From 5a611e91302cac64c3d78f28b665ac73b670b305 Mon Sep 17 00:00:00 2001
From: Henrik Smiding <henrik.smiding@intel.com>
Date: Wed, 18 Dec 2013 15:18:52 +0100
Subject: [PATCH 13/17] Add SSE4 optimization of S32A_Blend_Blitrow

BZ: 146848

Adds optimization of Skia S32A_Blend_Blitrow blitter using SSE4 SIMD
instruction set. Special case for when alpha is zero.

Change-Id: If77f57f97eafebe18d22918da2c66f519d304bf0
Category: aosp improvement
Domain: AOSP-Others
Origin: internal
Upstream-Candidate: yes
Signed-off-by: Henrik Smiding <henrik.smiding@intel.com>
---
 src/opts/SkBlitRow_opts_SSE4.h     |   4 +
 src/opts/SkBlitRow_opts_SSE4_asm.S | 623 +++++++++++++++++++++++++++++++++++++
 src/opts/opts_check_x86.cpp        |   3 +-
 3 files changed, 629 insertions(+), 1 deletion(-)

diff --git a/src/opts/SkBlitRow_opts_SSE4.h b/src/opts/SkBlitRow_opts_SSE4.h
index a053259..bd2ad2b 100644
--- a/src/opts/SkBlitRow_opts_SSE4.h
+++ b/src/opts/SkBlitRow_opts_SSE4.h
@@ -14,5 +14,9 @@ extern "C" void S32A_Opaque_BlitRow32_SSE4_asm(SkPMColor* SK_RESTRICT dst,
                                                const SkPMColor* SK_RESTRICT src,
                                                int count, U8CPU alpha);
 
+extern "C" void S32A_Blend_BlitRow32_SSE4_asm(SkPMColor* SK_RESTRICT dst,
+                                              const SkPMColor* SK_RESTRICT src,
+                                              int count, U8CPU alpha);
+
 #endif
 
diff --git a/src/opts/SkBlitRow_opts_SSE4_asm.S b/src/opts/SkBlitRow_opts_SSE4_asm.S
index ce1d806..62b0a7a 100644
--- a/src/opts/SkBlitRow_opts_SSE4_asm.S
+++ b/src/opts/SkBlitRow_opts_SSE4_asm.S
@@ -586,3 +586,626 @@ S32A_Opaque_BlitRow32_SSE4_asm:
 
     .cfi_endproc
     .size S32A_Opaque_BlitRow32_SSE4_asm, .-S32A_Opaque_BlitRow32_SSE4_asm
+
+
+/*
+ * void S32A_Blend_BlitRow32_SSE4(SkPMColor* SK_RESTRICT dst,
+ *                                const SkPMColor* SK_RESTRICT src,
+ *                                int count, U8CPU alpha)
+ *
+ * The primary optimization comes from checking the source pixels' alpha value.
+ * If the alpha is zero, the pixel can be skipped entirely.
+ * According to collected statistics, this case is quite common.
+ * The main loop(s) uses pre-loading and unrolling in an attempt to reduce the
+ * memory latency worse-case.
+ */
+
+    .section .text.sse4,"ax",@progbits
+    .type S32A_Blend_BlitRow32_SSE4_asm, @function
+    .globl S32A_Blend_BlitRow32_SSE4_asm
+
+    .p2align 4
+S32A_Blend_BlitRow32_SSE4_asm:
+    .cfi_startproc
+    PUSH(%ebx)
+    movl        12(%esp), %eax          // Source pointer
+    movl        16(%esp), %ecx          // Pixel count
+    movl        8(%esp), %edx           // Destination pointer
+    movl        20(%esp), %ebx          // Fetch global alpha parameter
+    prefetcht0  (%eax)
+
+    // Setup SSE constants
+    pcmpeqd     %xmm7, %xmm7            // 0xFF000000 mask to check alpha
+    pcmpeqw     %xmm6, %xmm6            // 16-bit 256 to calculate inv. alpha
+    pslld       $24, %xmm7
+    pcmpeqw     %xmm0, %xmm0            // 0x00FF00FF mask (Must be in xmm0 because of pblendvb)
+    addl        $1, %ebx                // Modify global alpha range to 1..256
+    psrlw       $15, %xmm6
+    imul        $0x10001, %ebx          // Duplicate alpha to two 16-bit values
+    psrlw       $8, %xmm0
+    subl        $4, %ecx                // Check if we have only 0-3 pixels
+    psllw       $8, %xmm6
+    js          .LBlendReallySmall
+    PUSH(%edi)
+    cmpl        $11, %ecx               // Do we have enough pixels to run the main loop?
+    ja          .LBlendBigBlit
+
+    // Handle small blits (4-15 pixels)
+    // ********************************
+    xorl        %edi, %edi              // Reset offset to zero
+
+.LBlendSmallLoop:
+    lddqu       (%eax, %edi), %xmm1     // Load four source pixels
+    ptest       %xmm7, %xmm1            // Check if all alphas are zero or opaque
+    jz          .LBlendSmallAlphaZero
+
+    // Handle global and pixel alphas (calculate and scale)
+    movd        %ebx, %xmm4             // Create global alpha constant
+    movdqa      %xmm0, %xmm3
+    pshufd      $0, %xmm4, %xmm4
+
+    pandn       %xmm1, %xmm3            // Filter out alpha and green components from source
+    lddqu       (%edx, %edi), %xmm5     // Load last four destination pixels (overlapping)
+    pmulhuw     %xmm4, %xmm3            // Scale alpha and green
+    psllw       $8, %xmm1               // Filter out red and blue components from source
+    pmulhuw     %xmm4, %xmm1            // Scale red and blue
+
+    pshufhw     $0xF5, %xmm3, %xmm2     // Repeat source alpha for scaling of destination (high)
+    psllw       $8, %xmm3               // Combine scaled source results
+    pshuflw     $0xF5, %xmm2, %xmm2     // Repeat source alpha (low)
+    movdqa      %xmm6, %xmm4            // Clone 256 constant
+    por         %xmm3, %xmm1
+    psubw       %xmm2, %xmm4            // Finalize alpha calculations
+    movdqa      %xmm5, %xmm3            // Clone destination pixels
+
+    psllw       $8, %xmm5               // Filter out red and blue components
+    pmulhuw     %xmm4, %xmm5            // Scale red and blue
+    psrlw       $8, %xmm3               // Filter out alpha and green components
+    pmullw      %xmm4, %xmm3            // Scale alpha and green
+
+    addl        $16, %edi
+    subl        $4, %ecx                // Check if we can store all four pixels
+    pblendvb    %xmm0, %xmm5, %xmm3     // Combine scaled destination results
+    paddb       %xmm3, %xmm1            // Add source and destination pixels together
+    movdqu      %xmm1, -16(%edx, %edi)  // Store four destination pixels
+    jns         .LBlendSmallLoop
+    jmp         .LBlendSmallRemaining
+
+    .p2align 4
+.LBlendSmallAlphaZero:
+    addl        $16, %edi
+    subl        $4, %ecx                // Check if there are four additional pixels, at least
+    jns         .LBlendSmallLoop
+
+    // Handle the last 0-3 pixels (also used by the big unaligned loop)
+.LBlendSmallRemaining:
+    cmpl        $-4, %ecx               // Check if we are done
+    je          .LBlendSmallExit
+    sall        $2, %ecx                // Calculate offset for last pixels
+    addl        %ecx, %edi
+
+    lddqu       (%eax, %edi), %xmm1     // Load last four source pixels (overlapping)
+    ptest       %xmm7, %xmm1            // Check if all alphas are zero or opaque
+    jz          .LBlendSmallExit
+
+    // Handle mixed alphas (calculate and scale)
+    movd        %ebx, %xmm4             // Create global alpha constant
+    movdqa      %xmm0, %xmm3
+    pshufd      $0, %xmm4, %xmm4
+
+    pandn       %xmm1, %xmm3            // Filter out alpha and green components from source
+    lddqu       (%edx, %edi), %xmm5     // Load last four destination pixels (overlapping)
+    pmulhuw     %xmm4, %xmm3            // Scale alpha and green
+    psllw       $8, %xmm1               // Filter out red and blue components from source
+    pmulhuw     %xmm4, %xmm1            // Scale red and blue
+
+    pshufhw     $0xF5, %xmm3, %xmm2     // Repeat source alpha for scaling of destination (high)
+    psllw       $8, %xmm3               // Combine scaled source results
+    pshuflw     $0xF5, %xmm2, %xmm2     // Repeat source alpha (low)
+    movdqa      %xmm6, %xmm4            // Clone 256 constant
+    por         %xmm3, %xmm1
+    psubw       %xmm2, %xmm4            // Finalize alpha calculations
+    movdqa      %xmm5, %xmm3            // Clone destination pixels
+
+    psllw       $8, %xmm3               // Filter out red and blue components
+    pmulhuw     %xmm4, %xmm3            // Scale red and blue
+    movdqa      %xmm5, %xmm2            // Clone destination pixels
+    psrlw       $8, %xmm2               // Filter out alpha and green components
+    pmullw      %xmm4, %xmm2            // Scale alpha and green
+
+    pblendvb    %xmm0, %xmm3, %xmm2     // Combine scaled destination results
+    cmp         $-8, %ecx               // Check how many pixels should be written
+    paddb       %xmm2, %xmm1            // Add source and destination pixels together
+    jb          .LBlendSmallPixelsLeft1
+    ja          .LBlendSmallPixelsLeft3
+    pblendw     $0xF0, %xmm1, %xmm5
+    movdqu      %xmm5, (%edx, %edi)     // Store last two destination pixels
+.LBlendSmallExit:
+    POP(%edi)
+    POP(%ebx)
+    ret
+
+.LBlendSmallPixelsLeft1:
+    pblendw     $0xC0, %xmm1, %xmm5
+    movdqu      %xmm5, (%edx, %edi)     // Store last destination pixel
+    POP(%edi)
+    POP(%ebx)
+    ret
+
+.LBlendSmallPixelsLeft3:
+    pblendw     $0xFC, %xmm1, %xmm5
+    movdqu      %xmm5, (%edx, %edi)     // Store last three destination pixels
+    POP(%edi)
+    POP(%ebx)
+    ret
+
+
+    // Handle really small blits (0-3 pixels)
+    // **************************************
+.LBlendReallySmall:
+    addl        $4, %ecx
+    jle         .LBlendReallySmallExit
+    pxor        %xmm1, %xmm1
+    cmp         $2, %ecx                // Check how many pixels should be read
+    pinsrd      $0x0, (%eax), %xmm1     // Load one source pixel
+    pinsrd      $0x0, (%edx), %xmm5     // Load one destination pixel
+    jb          .LBlendReallySmallCalc
+    pinsrd      $0x1, 4(%eax), %xmm1    // Load second source pixel
+    pinsrd      $0x1, 4(%edx), %xmm5    // Load second destination pixel
+    je          .LBlendReallySmallCalc
+    pinsrd      $0x2, 8(%eax), %xmm1    // Load third source pixel
+    pinsrd      $0x2, 8(%edx), %xmm5    // Load third destination pixel
+
+.LBlendReallySmallCalc:
+    ptest       %xmm7, %xmm1            // Check if all alphas are opaque
+    jz          .LBlendReallySmallExit  // If all alphas are zero, just store
+
+    // Handle mixed alphas (calculate and scale)
+    movd        %ebx, %xmm4             // Create global alpha constant
+    movdqa      %xmm0, %xmm3
+    pshufd      $0, %xmm4, %xmm4
+
+    pandn       %xmm1, %xmm3            // Filter out alpha and green components from source
+    pmulhuw     %xmm4, %xmm3            // Scale alpha and green
+    psllw       $8, %xmm1               // Filter out red and blue components from source
+    pmulhuw     %xmm4, %xmm1            // Scale red and blue
+
+    pshufhw     $0xF5, %xmm3, %xmm2     // Repeat source alpha for scaling of destination (high)
+    psllw       $8, %xmm3               // Combine scaled source results
+    pshuflw     $0xF5, %xmm2, %xmm2     // Repeat source alpha (low)
+    movdqa      %xmm6, %xmm4            // Clone 256 constant
+    por         %xmm3, %xmm1
+    psubw       %xmm2, %xmm4            // Finalize alpha calculations
+    movdqa      %xmm5, %xmm3            // Clone destination pixels
+
+    psllw       $8, %xmm5               // Filter out red and blue components
+    pmulhuw     %xmm4, %xmm5            // Scale red and blue
+    psrlw       $8, %xmm3               // Filter out alpha and green components
+    pmullw      %xmm4, %xmm3            // Scale alpha and green
+
+    pblendvb    %xmm0, %xmm5, %xmm3     // Combine scaled destination results
+    cmp         $2, %ecx                // Check how many pixels should be written
+    paddb       %xmm3, %xmm1            // Add source and destination pixels together
+    pextrd      $0x0, %xmm1, (%edx)     // Store one destination pixel
+    jb          .LBlendReallySmallExit
+    pextrd      $0x1, %xmm1, 4(%edx)    // Store second destination pixel
+    je          .LBlendReallySmallExit
+    pextrd      $0x2, %xmm1, 8(%edx)    // Store third destination pixel
+.LBlendReallySmallExit:
+    POP(%ebx)
+    ret
+
+    // Handle bigger blit operations (16+ pixels)
+    // ******************************************
+    .p2align 4
+.LBlendBigBlit:
+    // Align destination?
+    testl       $0xF, %edx
+    lddqu       (%eax), %xmm1           // Pre-load four source pixels
+    jz          .LBlendAligned
+
+    movl        %edx, %edi              // Calculate alignment of destination pointer
+    negl        %edi
+    andl        $0xF, %edi
+
+    // Handle 1-3 pixels to align destination
+    ptest       %xmm7, %xmm1            // Check if all alphas are zero or opaque
+    jz          .LBlendAlignDone        // If all alphas are opaque, just skip
+
+    // Handle global and pixel alphas (calculate and scale)
+    movd        %ebx, %xmm4             // Create global alpha constant
+    movdqa      %xmm0, %xmm3
+    pshufd      $0, %xmm4, %xmm4
+
+    pandn       %xmm1, %xmm3            // Filter out alpha and green components from source
+    lddqu       (%edx), %xmm5           // Load four destination pixels
+    pmulhuw     %xmm4, %xmm3            // Scale alpha and green
+    psllw       $8, %xmm1               // Filter out red and blue components from source
+    pmulhuw     %xmm4, %xmm1            // Scale red and blue
+
+    pshufhw     $0xF5, %xmm3, %xmm2     // Repeat source alpha for scaling of destination (high)
+    psllw       $8, %xmm3               // Combine scaled source results
+    pshuflw     $0xF5, %xmm2, %xmm2     // Repeat source alpha (low)
+    movdqa      %xmm6, %xmm4            // Clone 256 constant
+    por         %xmm3, %xmm1
+    psubw       %xmm2, %xmm4            // Finalize alpha calculations
+    movdqa      %xmm5, %xmm3            // Clone destination pixels
+
+    psllw       $8, %xmm3               // Filter out red and blue components
+    pmulhuw     %xmm4, %xmm3            // Scale red and blue
+    movdqa      %xmm5, %xmm2            // Clone destination pixels
+    psrlw       $8, %xmm2               // Filter out alpha and green components
+    pmullw      %xmm4, %xmm2            // Scale alpha and green
+
+    cmp         $8, %edi                // Check how many pixels should be written
+    pblendvb    %xmm0, %xmm3, %xmm2     // Combine scaled destination results
+    paddb       %xmm2, %xmm1            // Add source and destination pixels together
+    jb          .LBlendAlignPixelsLeft1
+    ja          .LBlendAlignPixelsLeft3
+    pblendw     $0x0F, %xmm1, %xmm5     // Blend two pixels
+    jmp .LBlendAlignStorePixels
+
+.LBlendAlignPixelsLeft1:
+    pblendw     $0x03, %xmm1, %xmm5     // Blend one pixel
+    jmp .LBlendAlignStorePixels
+
+.LBlendAlignPixelsLeft3:
+    pblendw     $0x3F, %xmm1, %xmm5     // Blend three pixels
+
+.LBlendAlignStorePixels:
+    movdqu      %xmm5, (%edx)           // Store destination pixels
+
+.LBlendAlignDone:
+    addl        %edi, %eax              // Adjust pointers and pixel count
+    addl        %edi, %edx
+    shrl        $2, %edi
+    lddqu       (%eax), %xmm1           // Pre-load new source pixels (after alignment)
+    subl        %edi, %ecx
+
+.LBlendAligned:                         // Destination is guaranteed to be 16 byte aligned
+    xorl        %edi, %edi              // Reset offset to zero
+    subl        $8, %ecx                // Decrease counter (Reserve four pixels for the cleanup)
+    testl       $0xF, %eax              // Check alignment of source pointer
+    jz          .LBlendAlignedLoop
+
+    // Source not aligned to destination
+    // *********************************
+    .p2align 4
+.LBlendUnalignedLoop:                   // Main loop for unaligned, handles eight pixels per iteration
+    ptest       %xmm7, %xmm1            // Check if all alphas are zero or opaque
+    jz          .LBlendAlphaZero00
+
+    // Handle global and pixel alphas (calculate and scale)
+.LBlendAlphaNotZero00:
+    movd        %ebx, %xmm4             // Create global alpha constant
+    movdqa      %xmm0, %xmm3
+    pshufd      $0, %xmm4, %xmm4
+
+    pandn       %xmm1, %xmm3            // Filter out alpha and green components from source
+    movdqa      (%edx, %edi), %xmm5     // Load four destination pixels
+    pmulhuw     %xmm4, %xmm3            // Scale alpha and green
+    psllw       $8, %xmm1               // Filter out red and blue components from source
+    pmulhuw     %xmm4, %xmm1            // Scale red and blue
+
+    pshufhw     $0xF5, %xmm3, %xmm2     // Repeat source alpha for scaling of destination (high)
+    psllw       $8, %xmm3               // Combine scaled source results
+    pshuflw     $0xF5, %xmm2, %xmm2     // Repeat source alpha (low)
+    movdqa      %xmm6, %xmm4            // Clone 256 constant
+    por         %xmm3, %xmm1
+    psubw       %xmm2, %xmm4            // Finalize alpha calculations
+    movdqa      %xmm5, %xmm3            // Clone destination pixels
+
+    psllw       $8, %xmm5               // Filter out red and blue components
+    pmulhuw     %xmm4, %xmm5            // Scale red and blue
+    psrlw       $8, %xmm3               // Filter out alpha and green components
+    pmullw      %xmm4, %xmm3            // Scale alpha and green
+
+    lddqu       16(%eax, %edi), %xmm2   // Pre-load four source pixels
+    pblendvb    %xmm0, %xmm5, %xmm3     // Combine scaled destination results
+    paddb       %xmm3, %xmm1            // Add source and destination pixels together
+    movdqa      %xmm1, (%edx, %edi)     // Store four destination pixels
+
+    // Handle next four pixels
+    ptest       %xmm7, %xmm2            // Check if all alphas are zero or opaque
+    jz         .LBlendAlphaZero01
+
+    // Handle global and pixel alphas (calculate and scale)
+.LBlendAlphaNotZero01:
+    movd        %ebx, %xmm4             // Create global alpha constant
+    movdqa      %xmm0, %xmm3
+    pshufd      $0, %xmm4, %xmm4
+
+    pandn       %xmm2, %xmm3            // Filter out alpha and green components from source
+    movdqa      16(%edx, %edi), %xmm5   // Load four destination pixels
+    pmulhuw     %xmm4, %xmm3            // Scale alpha and green
+    psllw       $8, %xmm2               // Filter out red and blue components from source
+    pmulhuw     %xmm4, %xmm2            // Scale red and blue
+
+    pshufhw     $0xF5, %xmm3, %xmm1     // Repeat source alpha for scaling of destination (high)
+    psllw       $8, %xmm3               // Combine scaled source results
+    pshuflw     $0xF5, %xmm1, %xmm1     // Repeat source alpha (low)
+    movdqa      %xmm6, %xmm4            // Clone 256 constant
+    por         %xmm3, %xmm2
+    psubw       %xmm1, %xmm4            // Finalize alpha calculations
+    movdqa      %xmm5, %xmm3            // Clone destination pixels
+
+    psllw       $8, %xmm5               // Filter out red and blue components
+    pmulhuw     %xmm4, %xmm5            // Scale red and blue
+    psrlw       $8, %xmm3               // Filter out alpha and green components
+    pmullw      %xmm4, %xmm3            // Scale alpha and green
+
+    lddqu       32(%eax, %edi), %xmm1   // Pre-load four source pixels
+    addl        $32, %edi
+    pblendvb    %xmm0, %xmm5, %xmm3     // Combine scaled destination results
+    paddb       %xmm3, %xmm2            // Add source and destination pixels together
+    subl        $8, %ecx
+    movdqa      %xmm2, -16(%edx, %edi)  // Store four destination pixels
+    jae         .LBlendUnalignedLoop
+    addl        $8, %ecx                // Adjust pixel count
+    jmp         .LBlendLoopCleanup0
+
+    .p2align 4
+.LBlendUnalignedLoopZero:               // Alternate 'alpha zero' main loop for unaligned
+    ptest       %xmm7, %xmm1            // Check if all alphas are zero or opaque
+    jnz         .LBlendAlphaNotZero00
+.LBlendAlphaZero00:
+    lddqu       16(%eax, %edi), %xmm2   // Pre-load four source pixels
+    ptest       %xmm7, %xmm2            // Check if all alphas are zero or opaque
+    jnz         .LBlendAlphaNotZero01
+.LBlendAlphaZero01:
+    lddqu       32(%eax, %edi), %xmm1   // Pre-load four source pixels
+    addl        $32, %edi               // Adjust offset and pixel count
+    subl        $8, %ecx
+    jae         .LBlendUnalignedLoopZero
+    addl        $8, %ecx                // Adjust pixel count
+
+    // Cleanup - handle pending pixels from loop
+.LBlendLoopCleanup0:
+    ptest       %xmm7, %xmm1            // Check if all alphas are zero or opaque
+    jz          .LBlendAlphaZero03
+
+    // Handle global and pixel alphas (calculate and scale)
+    movd        %ebx, %xmm4             // Create global alpha constant
+    movdqa      %xmm0, %xmm3
+    pshufd      $0, %xmm4, %xmm4
+
+    pandn       %xmm1, %xmm3            // Filter out alpha and green components from source
+    movdqa      (%edx, %edi), %xmm5     // Load four destination pixels
+    pmulhuw     %xmm4, %xmm3            // Scale alpha and green
+    psllw       $8, %xmm1               // Filter out red and blue components from source
+    pmulhuw     %xmm4, %xmm1            // Scale red and blue
+
+    pshufhw     $0xF5, %xmm3, %xmm2     // Repeat source alpha for scaling of destination (high)
+    psllw       $8, %xmm3               // Combine scaled source results
+    pshuflw     $0xF5, %xmm2, %xmm2     // Repeat source alpha (low)
+    movdqa      %xmm6, %xmm4            // Clone 256 constant
+    por         %xmm3, %xmm1
+    psubw       %xmm2, %xmm4            // Finalize alpha calculations
+    movdqa      %xmm5, %xmm3            // Clone destination pixels
+
+    psllw       $8, %xmm5               // Filter out red and blue components
+    pmulhuw     %xmm4, %xmm5            // Scale red and blue
+    psrlw       $8, %xmm3               // Filter out alpha and green components
+    pmullw      %xmm4, %xmm3            // Scale alpha and green
+
+    pblendvb    %xmm0, %xmm5, %xmm3     // Combine scaled destination results
+    paddb       %xmm3, %xmm1            // Add source and destination pixels together
+    movdqa      %xmm1, (%edx, %edi)     // Store four destination pixels
+
+.LBlendAlphaZero03:
+    addl        $16, %edi
+    subl        $4, %ecx
+    js          .LBlendSmallRemaining   // Reuse code from small loop
+    lddqu       (%eax, %edi), %xmm1     // Pre-load four source pixels
+    jmp         .LBlendLoopCleanup0
+
+    // Source aligned to destination
+    // *****************************
+    .p2align 4
+.LBlendAlignedLoop:                     // Main loop for aligned, handles eight pixels per iteration
+    ptest       %xmm7, %xmm1            // Check if all alphas are zero or opaque
+    jz          .LBlendAlphaZero10
+
+    // Handle global and pixel alphas (calculate and scale)
+.LBlendAlphaNotZero10:
+    movd        %ebx, %xmm4             // Create global alpha constant
+    movdqa      %xmm0, %xmm3
+    pshufd      $0, %xmm4, %xmm4
+
+    pandn       %xmm1, %xmm3            // Filter out alpha and green components from source
+    movdqa      (%edx, %edi), %xmm5     // Load four destination pixels
+    pmulhuw     %xmm4, %xmm3            // Scale alpha and green
+    psllw       $8, %xmm1               // Filter out red and blue components from source
+    pmulhuw     %xmm4, %xmm1            // Scale red and blue
+
+    pshufhw     $0xF5, %xmm3, %xmm2     // Repeat source alpha for scaling of destination (high)
+    psllw       $8, %xmm3               // Combine scaled source results
+    pshuflw     $0xF5, %xmm2, %xmm2     // Repeat source alpha (low)
+    movdqa      %xmm6, %xmm4            // Clone 256 constant
+    por         %xmm3, %xmm1
+    psubw       %xmm2, %xmm4            // Finalize alpha calculations
+    movdqa      %xmm5, %xmm3            // Clone destination pixels
+
+    psllw       $8, %xmm5               // Filter out red and blue components
+    pmulhuw     %xmm4, %xmm5            // Scale red and blue
+    psrlw       $8, %xmm3               // Filter out alpha and green components
+    pmullw      %xmm4, %xmm3            // Scale alpha and green
+
+    movdqa      16(%eax, %edi), %xmm2   // Pre-load four source pixels
+    pblendvb    %xmm0, %xmm5, %xmm3     // Combine scaled destination results
+    paddb       %xmm3, %xmm1            // Add source and destination pixels together
+    movdqa      %xmm1, (%edx, %edi)     // Store four destination pixels
+
+    // Handle next four pixels
+    ptest       %xmm7, %xmm2            // Check if all alphas are zero or opaque
+    jz         .LBlendAlphaZero11
+
+    // Handle global and pixel alphas (calculate and scale)
+.LBlendAlphaNotZero11:
+    movd        %ebx, %xmm4             // Create global alpha constant
+    movdqa      %xmm0, %xmm3
+    pshufd      $0, %xmm4, %xmm4
+
+    pandn       %xmm2, %xmm3            // Filter out alpha and green components from source
+    movdqa      16(%edx, %edi), %xmm5   // Load four destination pixels
+    pmulhuw     %xmm4, %xmm3            // Scale alpha and green
+    psllw       $8, %xmm2               // Filter out red and blue components from source
+    pmulhuw     %xmm4, %xmm2            // Scale red and blue
+
+    pshufhw     $0xF5, %xmm3, %xmm1     // Repeat source alpha for scaling of destination (high)
+    psllw       $8, %xmm3               // Combine scaled source results
+    pshuflw     $0xF5, %xmm1, %xmm1     // Repeat source alpha (low)
+    movdqa      %xmm6, %xmm4            // Clone 256 constant
+    por         %xmm3, %xmm2
+    psubw       %xmm1, %xmm4            // Finalize alpha calculations
+    movdqa      %xmm5, %xmm3            // Clone destination pixels
+
+    psllw       $8, %xmm5               // Filter out red and blue components
+    pmulhuw     %xmm4, %xmm5            // Scale red and blue
+    psrlw       $8, %xmm3               // Filter out alpha and green components
+    pmullw      %xmm4, %xmm3            // Scale alpha and green
+
+    movdqa      32(%eax, %edi), %xmm1   // Pre-load four source pixels
+    addl        $32, %edi
+    pblendvb    %xmm0, %xmm5, %xmm3     // Combine scaled destination results
+    paddb       %xmm3, %xmm2            // Add source and destination pixels together
+    subl        $8, %ecx
+    movdqa      %xmm2, -16(%edx, %edi)  // Store four destination pixels
+    jae         .LBlendAlignedLoop
+    jmp         .LBlendLoopCleanup1
+
+    .p2align 4
+.LBlendAlignedLoopZero:                 // Alternate 'alpha zero' main loop for unaligned
+    ptest       %xmm7, %xmm1            // Check if all alphas are zero or opaque
+    jnz         .LBlendAlphaNotZero10
+.LBlendAlphaZero10:
+    movdqa      16(%eax, %edi), %xmm2   // Pre-load four source pixels
+    ptest       %xmm7, %xmm2            // Check if all alphas are zero or opaque
+    jnz         .LBlendAlphaNotZero11
+.LBlendAlphaZero11:
+    movdqa      32(%eax, %edi), %xmm1   // Pre-load four source pixels
+    addl        $32, %edi               // Adjust offset and pixel count
+    subl        $8, %ecx
+    jae         .LBlendAlignedLoopZero
+
+    // Cleanup - handle four pending pixels from loop
+.LBlendLoopCleanup1:
+    ptest       %xmm7, %xmm1            // Check if all alphas are zero or opaque
+    jz          .LBlendAlphaZero13
+
+    // Handle global and pixel alphas (calculate and scale)
+    movd        %ebx, %xmm4             // Create global alpha constant
+    movdqa      %xmm0, %xmm3
+    pshufd      $0, %xmm4, %xmm4
+
+    pandn       %xmm1, %xmm3            // Filter out alpha and green components from source
+    movdqa      (%edx, %edi), %xmm5     // Load four destination pixels
+    pmulhuw     %xmm4, %xmm3            // Scale alpha and green
+    psllw       $8, %xmm1               // Filter out red and blue components from source
+    pmulhuw     %xmm4, %xmm1            // Scale red and blue
+
+    pshufhw     $0xF5, %xmm3, %xmm2     // Repeat source alpha for scaling of destination (high)
+    psllw       $8, %xmm3               // Combine scaled source results
+    pshuflw     $0xF5, %xmm2, %xmm2     // Repeat source alpha (low)
+    movdqa      %xmm6, %xmm4            // Clone 256 constant
+    por         %xmm3, %xmm1
+    psubw       %xmm2, %xmm4            // Finalize alpha calculations
+    movdqa      %xmm5, %xmm3            // Clone destination pixels
+
+    psllw       $8, %xmm5               // Filter out red and blue components
+    pmulhuw     %xmm4, %xmm5            // Scale red and blue
+    psrlw       $8, %xmm3               // Filter out alpha and green components
+    pmullw      %xmm4, %xmm3            // Scale alpha and green
+
+    pblendvb    %xmm0, %xmm5, %xmm3     // Combine scaled destination results
+    paddb       %xmm3, %xmm1            // Add source and destination pixels together
+    movdqa      %xmm1, (%edx, %edi)     // Store four destination pixels
+
+.LBlendAlphaZero13:
+    addl        $8, %ecx                // Adjust offset and pixel count
+    jz          .LBlendExit
+    addl        $16, %edi
+
+    // Handle last 1-7 pixels
+.LBlendRemainLoop1:
+    movdqa      (%eax, %edi), %xmm1     // Load four source pixels
+    ptest       %xmm7, %xmm1            // Check if all alphas are zero or opaque
+    jz          .LBlendRemainAlphaZero1
+
+    // Handle global and pixel alphas (calculate and scale)
+    movd        %ebx, %xmm4             // Create global alpha constant
+    movdqa      %xmm0, %xmm3
+    pshufd      $0, %xmm4, %xmm4
+
+    pandn       %xmm1, %xmm3            // Filter out alpha and green components from source
+    movdqa      (%edx, %edi), %xmm5     // Load four destination pixels
+    pmulhuw     %xmm4, %xmm3            // Scale alpha and green
+    psllw       $8, %xmm1               // Filter out red and blue components from source
+    pmulhuw     %xmm4, %xmm1            // Scale red and blue
+
+    pshufhw     $0xF5, %xmm3, %xmm2     // Repeat source alpha for scaling of destination (high)
+    psllw       $8, %xmm3               // Combine scaled source results
+    pshuflw     $0xF5, %xmm2, %xmm2     // Repeat source alpha (low)
+    movdqa      %xmm6, %xmm4            // Clone 256 constant
+    por         %xmm3, %xmm1
+    psubw       %xmm2, %xmm4            // Finalize alpha calculations
+    movdqa      %xmm5, %xmm3            // Clone destination pixels
+
+    psllw       $8, %xmm5               // Filter out red and blue components
+    pmulhuw     %xmm4, %xmm5            // Scale red and blue
+    psrlw       $8, %xmm3               // Filter out alpha and green components
+    pmullw      %xmm4, %xmm3            // Scale alpha and green
+
+    subl        $4, %ecx
+    pblendvb    %xmm0, %xmm5, %xmm3     // Combine scaled destination results
+    paddb       %xmm3, %xmm1            // Add source and destination pixels together
+    jle         .LBlendRemainStore
+    movdqa      %xmm1, (%edx, %edi)     // Store four destination pixels
+    addl        $16, %edi
+    jmp         .LBlendRemainLoop1
+
+    // All alphas were zero (skip)
+    .p2align 4
+.LBlendRemainAlphaZero1:
+    subl        $4, %ecx                // Check if we have more than four pixels left
+    jle         .LBlendExit
+    addl        $16, %edi
+    jmp         .LBlendRemainLoop1
+
+    // Store the last 1-4 pixels
+    .p2align 4
+.LBlendRemainStore:
+    jz          .LBlendRemainFull
+    movdqa      (%edx, %edi), %xmm5     // Load four destination pixels
+    cmp         $-2, %ecx               // Check how many pixels should be written
+    jb          .LBlendRemainPixelsLeft11
+    ja          .LBlendRemainPixelsLeft13
+    pblendw     $0x0F, %xmm1, %xmm5
+    movdqa      %xmm5, (%edx, %edi)     // Store last 2 destination pixels
+.LBlendExit:
+    POP(%edi)                           // Exit
+    POP(%ebx)
+    ret
+
+.LBlendRemainPixelsLeft11:
+    pblendw     $0x03, %xmm1, %xmm5
+    movdqa      %xmm5, (%edx, %edi)     // Store last destination pixel
+    POP(%edi)                           // Exit
+    POP(%ebx)
+    ret
+
+.LBlendRemainPixelsLeft13:
+    pblendw     $0x3F, %xmm1, %xmm5
+    movdqa      %xmm5, (%edx, %edi)     // Store last 3 destination pixels
+    POP(%edi)                           // Exit
+    POP(%ebx)
+    ret
+
+.LBlendRemainFull:
+    movdqa      %xmm1, (%edx, %edi)     // Store last 4 destination pixels
+    POP(%edi)                           // Exit
+    POP(%ebx)
+    ret
+
+    .cfi_endproc
+    .size S32A_Blend_BlitRow32_SSE4_asm, .-S32A_Blend_BlitRow32_SSE4_asm
diff --git a/src/opts/opts_check_x86.cpp b/src/opts/opts_check_x86.cpp
index 6662ad6..1099bb9 100644
--- a/src/opts/opts_check_x86.cpp
+++ b/src/opts/opts_check_x86.cpp
@@ -238,11 +238,12 @@ static SkBlitRow::Proc32 platform_32_procs_SSE4[] = {
     S32_Blend_BlitRow32_SSE2,           // S32_Blend
 #if !defined(__x86_64__)
     S32A_Opaque_BlitRow32_SSE4_asm,     // S32A_Opaque (32-bit assembly version)
+    S32A_Blend_BlitRow32_SSE4_asm       // S32A_Blend (32-bit assembly version)
 #else
 #warning "Can't use SSE4 assembly optimizations in 64-bit mode. Using old intrinsic version."
     S32A_Opaque_BlitRow32_SSE2,         // S32A_Opaque (Intrinsics fallback version)
+    S32A_Blend_BlitRow32_SSE2           // S32A_Blend (Intrinsics fallback version)
 #endif
-    S32A_Blend_BlitRow32_SSE2           // S32A_Blend
 };
 
 SkBlitRow::Proc32 SkBlitRow::PlatformProcs32(unsigned flags) {
-- 
2.7.4

