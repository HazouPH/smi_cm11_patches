From 3b1a0ada4afb01ce9bb0dacd85dc5a2b7c49cddb Mon Sep 17 00:00:00 2001
From: jhan16 <junchao.han@intel.com>
Date: Wed, 7 Aug 2013 03:45:17 +0800
Subject: [PATCH 04/17] SSE2 optimization for four skia basic functions

BZ: 68426

gerrit#79326
S32_D16_filter_DX_SSE2, S32_opaque_D32_nofilter_DX_SSE
Just to optimizae picel value recomputation routines when scaling bitmap
S32_opaque_D32_nofilter_DX_SSE2 is written with assembly.
The main contributor is Lei L Li in SOTC

S32_opaque_D32_filter_DXDY, S32_alpha_D32_filter_DXDY
the two functions are further optimized with assembly.
The optimization is targeted to HTML5 canvas 2D benchmark (CanvasPerf)
The main contributor is Yaojie Yan in SOTC

gerrit#80730
Port Fix camera taking picture crash from otc-ics ANDROID-1738,ANDROID-2073

Change-Id: I9c1e693331e12c0e3191e100b62a8f45773ef29b
Category: aosp improvement
Domain: AOSP.Optimization-Browser
Origin: internal
Upstream-Candidate: no, key fix
Author: Lei Li <lei.l.li@intel.com>
Orig-ABTChange-Id: Ica461a53aad7ff93f0b066241538f3ca469dc3b3
Signed-off-by: Jin Yang <jin.a.yang@intel.com>
---
 Android.mk                                 |    1 +
 src/core/SkBitmapProcShader.cpp            |    4 +-
 src/core/SkBitmapProcState.h               |    4 +
 src/opts/SkBitmapProcState_opts_SSE2.cpp   | 1089 +++++++++++++++++++++++++++-
 src/opts/SkBitmapProcState_opts_SSE2.h     |   16 +
 src/opts/SkBitmapProcState_opts_SSE2_asm.S |  451 ++++++++++++
 src/opts/opts_check_SSE2.cpp               |    6 +
 7 files changed, 1569 insertions(+), 2 deletions(-)
 create mode 100644 src/opts/SkBitmapProcState_opts_SSE2_asm.S

diff --git a/Android.mk b/Android.mk
index 84df891..375938e 100644
--- a/Android.mk
+++ b/Android.mk
@@ -560,6 +560,7 @@ LOCAL_SRC_FILES += \
     src/opts/SkBitmapFilter_opts_SSE2.cpp \
     src/opts/SkBlitRow_opts_SSE2.cpp \
     src/opts/SkBitmapProcState_opts_SSE2.cpp \
+    src/opts/SkBitmapProcState_opts_SSE2_asm.S \
     src/opts/SkBitmapProcState_opts_SSSE3.cpp \
     src/opts/SkBlitRect_opts_SSE2.cpp \
     src/opts/opts_check_SSE2.cpp \
diff --git a/src/core/SkBitmapProcShader.cpp b/src/core/SkBitmapProcShader.cpp
index 2521b56..917b251 100644
--- a/src/core/SkBitmapProcShader.cpp
+++ b/src/core/SkBitmapProcShader.cpp
@@ -196,7 +196,9 @@ void SkBitmapProcShader::shadeSpan(int x, int y, SkPMColor dstC[], int count) {
         return;
     }
 
-    uint32_t buffer[BUF_MAX + TEST_BUFFER_EXTRA];
+    uint32_t buffer1[BUF_MAX + TEST_BUFFER_EXTRA+ 16];
+    uint32_t* buffer = (uint32_t*)(((size_t)buffer1+ 0xF) &(~0xF));
+
     SkBitmapProcState::MatrixProc   mproc = state.getMatrixProc();
     SkBitmapProcState::SampleProc32 sproc = state.getSampleProc32();
     int max = fState.maxCountForBufferSize(sizeof(buffer[0]) * BUF_MAX);
diff --git a/src/core/SkBitmapProcState.h b/src/core/SkBitmapProcState.h
index 95daa82..f06c4e7 100644
--- a/src/core/SkBitmapProcState.h
+++ b/src/core/SkBitmapProcState.h
@@ -223,6 +223,10 @@ void S32_opaque_D32_filter_DXDY(const SkBitmapProcState& s,
                                 const uint32_t xy[], int count, SkPMColor colors[]);
 void S32_alpha_D32_filter_DXDY(const SkBitmapProcState& s,
                                const uint32_t xy[], int count, SkPMColor colors[]);
+void S32_D16_filter_DX(const SkBitmapProcState& s, const uint32_t xy[],
+                              int count, uint16_t colors[]);
+void S32_opaque_D32_nofilter_DX(const SkBitmapProcState& s, const uint32_t xy[],
+                             int count, SkPMColor colors[]);
 void ClampX_ClampY_filter_scale(const SkBitmapProcState& s, uint32_t xy[],
                                 int count, int x, int y);
 void ClampX_ClampY_nofilter_scale(const SkBitmapProcState& s, uint32_t xy[],
diff --git a/src/opts/SkBitmapProcState_opts_SSE2.cpp b/src/opts/SkBitmapProcState_opts_SSE2.cpp
index 0b07997..1572e0c 100644
--- a/src/opts/SkBitmapProcState_opts_SSE2.cpp
+++ b/src/opts/SkBitmapProcState_opts_SSE2.cpp
@@ -11,6 +11,13 @@
 #include "SkBitmapProcState_opts_SSE2.h"
 #include "SkPaint.h"
 #include "SkUtils.h"
+#include "SkBitmapProcState_filter.h"
+
+__attribute__((aligned(16)))
+uint32_t vf[4] = {0xF,0xf,0xf,0xf};
+uint32_t vmask2[4] = {0xff00ff00,0xff00ff00,0xff00ff00,0xff00ff00};
+unsigned short v256[8] = {256,256,256,256,256,256,256,256};
+uint32_t vmask[4] = {0x00ff00ff,0x00ff00ff,0x00ff00ff,0x00ff00ff};
 
 void S32_opaque_D32_filter_DX_SSE2(const SkBitmapProcState& s,
                                    const uint32_t* xy,
@@ -487,9 +494,13 @@ void ClampX_ClampY_nofilter_scale_SSE2(const SkBitmapProcState& s,
 
 /*  SSE version of ClampX_ClampY_filter_affine()
  *  portable version is in core/SkBitmapProcState_matrix.h
+ *  the address of xy should be 16bytes aligned, otherwise it will
+ *  core dump because of _mm_store_si128()
  */
 void ClampX_ClampY_filter_affine_SSE2(const SkBitmapProcState& s,
                                       uint32_t xy[], int count, int x, int y) {
+
+    SkASSERT(((size_t)xy & 0x0F) == 0);
     SkPoint srcPt;
     s.fInvProc(s.fInvMatrix,
                SkIntToScalar(x) + SK_ScalarHalf,
@@ -744,7 +755,7 @@ void S32_D16_filter_DX_SSE2(const SkBitmapProcState& s,
         // Extract low int and store.
         dstColor = _mm_cvtsi128_si32(sum);
 
-        //*colors++ = SkPixel32ToPixel16(dstColor);
+        // *colors++ = SkPixel32ToPixel16(dstColor);
         // below is much faster than the above. It's tested for Android benchmark--Softweg
         __m128i _m_temp1 = _mm_set1_epi32(dstColor);
         __m128i _m_temp2 = _mm_srli_epi32(_m_temp1, 3);
@@ -764,3 +775,1079 @@ void S32_D16_filter_DX_SSE2(const SkBitmapProcState& s,
 
     } while (--count > 0);
 }
+
+extern "C" void S32_opaque_D32_nofilter_DX_SSE2_asm(const uint32_t* xy,
+                                                    int count,
+                                                    const SkPMColor* srcAddr,
+                                                    uint32_t* colors);
+
+void S32_opaque_D32_nofilter_DX_SSE2(const SkBitmapProcState& s,
+                                     const uint32_t* xy,
+                                     int count, uint32_t* colors) {
+    SkASSERT(count > 0 && colors != NULL);
+    SkASSERT(s.fInvType <= (SkMatrix::kTranslate_Mask | SkMatrix::kScale_Mask));
+    SkASSERT(s.fDoFilter == false);
+    SkASSERT(s.fBitmap->config() == SkBitmap::kARGB_8888_Config);
+    SkASSERT(s.fAlphaScale == 256);
+
+    const SkPMColor* SK_RESTRICT srcAddr =
+        (const SkPMColor*)s.fBitmap->getPixels();
+
+    // buffer is y32, x16, x16, x16, x16, x16
+    // bump srcAddr to the proper row, since we're told Y never changes
+    SkASSERT((unsigned)xy[0] < (unsigned)s.fBitmap->height());
+    srcAddr = (const SkPMColor*)((const char*)srcAddr +
+                                                xy[0] * s.fBitmap->rowBytes());
+    xy += 1;
+
+    SkPMColor src;
+
+    if (1 == s.fBitmap->width()) {
+        src = srcAddr[0];
+        uint32_t dstValue = src;
+        sk_memset32(colors, dstValue, count);
+    } else {
+        int i;
+
+        S32_opaque_D32_nofilter_DX_SSE2_asm(xy, count, srcAddr, colors);
+
+        xy     += 2 * (count >> 2);
+        colors += 4 * (count >> 2);
+        const uint16_t* SK_RESTRICT xx = (const uint16_t*)(xy);
+        for (i = (count & 3); i > 0; --i) {
+            SkASSERT(*xx < (unsigned)s.fBitmap->width());
+            src = srcAddr[*xx++]; *colors++ = src;
+        }
+    }
+}
+
+void S32_opaque_D32_filter_DXDY_SSE2(const SkBitmapProcState& s,
+                                  const uint32_t* xy,
+                                  int count, uint32_t* colors) {
+
+    SkASSERT(count > 0 && colors != NULL);
+    SkASSERT(s.fDoFilter);
+    SkASSERT(s.fBitmap->config() == SkBitmap::kARGB_8888_Config);
+    SkASSERT(s.fAlphaScale == 256);
+    uint32_t data;
+    unsigned y0, y1, x0, x1, subX, subY;
+    const SkPMColor *row0, *row1;
+
+    const char* srcAddr = static_cast<const char*>(s.fBitmap->getPixels());
+    unsigned rb = s.fBitmap->rowBytes();
+    if (count >= 4) {
+        while (((size_t)xy & 0x0F) != 0)
+        {
+            data = *xy++;
+            y0 = data >> 14;
+            y1 = data & 0x3FFF;
+            subY = y0 & 0xF;
+            y0 >>= 4;
+
+            data = *xy++;
+            x0 = data >> 14;
+            x1 = data & 0x3FFF;
+            subX = x0 & 0xF;
+            x0 >>= 4;
+
+            row0 = (const SkPMColor*)(srcAddr + y0 * rb);
+            row1 = (const SkPMColor*)(srcAddr + y1 * rb);
+
+            Filter_32_opaque(subX, subY,
+                       (row0[x0]),
+                       (row0[x1]),
+                       (row1[x0]),
+                       (row1[x1]),
+                       colors);
+            colors += 1;
+            --count;
+        }
+        __m128i vf = _mm_set1_epi32(0xF);
+        __m128i vmask = _mm_set1_epi32(gMask_00FF00FF);
+        __m128i vmask2 = _mm_set1_epi32(0xff00ff00);
+        __m128i v256 = _mm_set1_epi16(256);
+        __m128i *d = reinterpret_cast<__m128i*>(colors);
+        while (count >= 4) {
+            __m128i vy_d = _mm_load_si128((__m128i*)xy);
+            __m128i vx_d = _mm_load_si128((__m128i*)(xy+4));
+            __m128i vy = (__m128i)_mm_shuffle_ps((__m128)vy_d,(__m128)vx_d,0x88);
+            __m128i vx = (__m128i)_mm_shuffle_ps((__m128)vy_d,(__m128)vx_d,0xdd);
+
+            uint32_t XY = *xy++;
+            const uint32_t* row0 = (const uint32_t*)(srcAddr + (XY >> 18) * rb);
+            const uint32_t* row1 = (const uint32_t*)(srcAddr + (XY & 0x3FFF) * rb);
+
+            uint32_t XX = *xy++;    // x0:14 | 4 | x1:14
+            unsigned x0 = XX >> 18;
+            unsigned x1 = XX & 0x3FFF;
+
+            __m128i a00 = _mm_cvtsi32_si128(row0[x0]);
+            __m128i a01 = _mm_cvtsi32_si128(row0[x1]);
+            __m128i a10 = _mm_cvtsi32_si128(row1[x0]);
+            __m128i a11 = _mm_cvtsi32_si128(row1[x1]);
+
+            XY = *xy++;
+            row0 = (const uint32_t*)(srcAddr + (XY >> 18) * rb);
+            row1 = (const uint32_t*)(srcAddr + (XY & 0x3FFF) * rb);
+
+            XX = *xy++;    // x0:14 | 4 | x1:14
+            x0 = XX >> 18;
+            x1 = XX & 0x3FFF;
+            a00 = _mm_unpacklo_epi32(a00,_mm_cvtsi32_si128(row0[x0]));
+            a01 = _mm_unpacklo_epi32(a01,_mm_cvtsi32_si128(row0[x1]));
+            a10 = _mm_unpacklo_epi32(a10,_mm_cvtsi32_si128(row1[x0]));
+            a11 = _mm_unpacklo_epi32(a11,_mm_cvtsi32_si128(row1[x1]));
+
+            XY = *xy++;
+            row0 = (const uint32_t*)(srcAddr + (XY >> 18) * rb);
+            row1 = (const uint32_t*)(srcAddr + (XY & 0x3FFF) * rb);
+
+            XX = *xy++;    // x0:14 | 4 | x1:14
+            x0 = XX >> 18;
+            x1 = XX & 0x3FFF;
+            __m128i a00_d = _mm_cvtsi32_si128(row0[x0]);
+            __m128i a01_d = _mm_cvtsi32_si128(row0[x1]);
+            __m128i a10_d = _mm_cvtsi32_si128(row1[x0]);
+            __m128i a11_d = _mm_cvtsi32_si128(row1[x1]);
+
+            XY = *xy++;
+            row0 = (const uint32_t*)(srcAddr + (XY >> 18) * rb);
+            row1 = (const uint32_t*)(srcAddr + (XY & 0x3FFF) * rb);
+
+            XX = *xy++;    // x0:14 | 4 | x1:14
+            x0 = XX >> 18;
+            x1 = XX & 0x3FFF;
+            a00_d = _mm_unpacklo_epi32(a00_d,_mm_cvtsi32_si128(row0[x0]));
+            a01_d = _mm_unpacklo_epi32(a01_d,_mm_cvtsi32_si128(row0[x1]));
+            a10_d = _mm_unpacklo_epi32(a10_d,_mm_cvtsi32_si128(row1[x0]));
+            a11_d = _mm_unpacklo_epi32(a11_d,_mm_cvtsi32_si128(row1[x1]));
+
+            vy = _mm_srli_epi32(vy,14);
+            vy = _mm_and_si128(vy,vf);
+
+            vx = _mm_srli_epi32(vx,14);
+            vx = _mm_and_si128(vx,vf);
+
+            a00 = _mm_unpacklo_epi64(a00,a00_d);
+            a01 = _mm_unpacklo_epi64(a01,a01_d);
+            a10 = _mm_unpacklo_epi64(a10,a10_d);
+            a11 = _mm_unpacklo_epi64(a11,a11_d);
+
+            vy = _mm_shufflelo_epi16(vy,0xa0);
+            vy = _mm_shufflehi_epi16(vy,0xa0);
+            vx = _mm_shufflelo_epi16(vx,0xa0);
+            vx = _mm_shufflehi_epi16(vx,0xa0);
+
+            // unsigned xy = x * y;
+            __m128i vxy = _mm_mullo_epi16(vx,vy);
+            __m128i v16y = _mm_slli_epi16(vy,4);
+            __m128i v16x = _mm_slli_epi16(vx,4);
+            // unsigned scale = 256 - 16*y - 16*x + xy;
+            __m128i vscale = _mm_add_epi16(v256,vxy);
+            vscale = _mm_sub_epi16(vscale,v16y);
+            vscale = _mm_sub_epi16(vscale,v16x);
+
+            // uint32_t lo = (a00 & mask) * scale;
+            __m128i vlo = _mm_and_si128(a00,vmask);
+            vlo = _mm_mullo_epi16(vlo, vscale);
+
+            // uint32_t hi = ((a00 >> 8) & mask) * scale;
+            __m128i vhi = _mm_srli_epi32(a00,8);
+            vhi = _mm_and_si128(vhi,vmask);
+            vhi = _mm_mullo_epi16(vhi, vscale);
+            // scale = 16*x-xy;
+            vscale = _mm_sub_epi16(v16x,vxy);
+
+            // lo += (a01 & mask) * scale;
+            __m128i vlo2 = _mm_and_si128(a01,vmask);
+            vlo2 = _mm_mullo_epi16(vlo2, vscale);
+            vlo = _mm_add_epi16(vlo,vlo2);
+
+            // hi += ((a01 >> 8) & mask) * scale;
+            __m128i vhi2 = _mm_srli_epi32(a01,8);
+            vhi2 = _mm_and_si128(vhi2,vmask);
+            vhi2 = _mm_mullo_epi16(vhi2, vscale);
+            vhi = _mm_add_epi16(vhi,vhi2);
+
+            // scale = 16*y - xy;
+            vscale = _mm_sub_epi16(v16y,vxy);
+
+            // lo += (a10 & mask) * scale;
+            vlo2 = _mm_and_si128(a10,vmask);
+            vlo2 = _mm_mullo_epi16(vlo2, vscale);
+            vlo = _mm_add_epi16(vlo,vlo2);
+
+            // hi += ((a10 >> 8) & mask) * scale;
+            vhi2 = _mm_srli_epi32(a10,8);
+            vhi2 = _mm_and_si128(vhi2,vmask);
+            vhi2 = _mm_mullo_epi16(vhi2, vscale);
+            vhi = _mm_add_epi16(vhi,vhi2);
+
+            // lo += (a11 & mask) * xy;
+            vlo2 = _mm_and_si128(a11,vmask);
+            vlo2 = _mm_mullo_epi16(vlo2, vxy);
+            vlo = _mm_add_epi16(vlo,vlo2);
+
+            // hi += ((a11 >> 8) & mask) * xy;
+            vhi2 = _mm_srli_epi32(a11,8);
+            vhi2 = _mm_and_si128(vhi2,vmask);
+            vhi2 = _mm_mullo_epi16(vhi2, vxy);
+            vhi = _mm_add_epi16(vhi,vhi2);
+
+            // *dstColor = ((lo >> 8) & mask) | (hi & ~mask);
+            vlo = _mm_srli_epi32(vlo,8);
+            vlo = _mm_and_si128(vlo,vmask);
+            vhi = _mm_and_si128(vhi,vmask2);
+
+            _mm_storeu_si128(d,_mm_or_si128(vlo,vhi));
+            d++;
+            count -= 4;
+        }
+    colors = reinterpret_cast<SkPMColor*>(d);
+    }
+    while (count > 0)
+    {
+        data = *xy++;
+        y0 = data >> 14;
+        y1 = data & 0x3FFF;
+        subY = y0 & 0xF;
+        y0 >>= 4;
+
+        data = *xy++;
+        x0 = data >> 14;
+        x1 = data & 0x3FFF;
+        subX = x0 & 0xF;
+        x0 >>= 4;
+
+        row0 = (const SkPMColor*)(srcAddr + y0 * rb);
+        row1 = (const SkPMColor*)(srcAddr + y1 * rb);
+
+        Filter_32_opaque(subX, subY,
+                   (row0[x0]),
+                   (row0[x1]),
+                   (row1[x0]),
+                   (row1[x1]),
+                   colors);
+        colors += 1;
+        count --;
+   }
+
+}
+
+void S32_opaque_D32_filter_DXDY_SSE2_asm(const SkBitmapProcState& s,
+        const uint32_t* xy, int count, uint32_t* colors) {
+    const char* srcAddr = static_cast<const char*>(s.fBitmap->getPixels());
+    unsigned rb = s.fBitmap->rowBytes();
+    uint32_t data;
+    unsigned y0, y1, x0, x1, subX, subY;
+    const SkPMColor *row0, *row1;
+    if (count >= 4) {
+        while (((size_t)xy & 0x0F) != 0)
+        {
+            data = *xy++;
+            y0 = data >> 14;
+            y1 = data & 0x3FFF;
+            subY = y0 & 0xF;
+            y0 >>= 4;
+
+            data = *xy++;
+            x0 = data >> 14;
+            x1 = data & 0x3FFF;
+            subX = x0 & 0xF;
+            x0 >>= 4;
+
+            row0 = (const SkPMColor*)(srcAddr + y0 * rb);
+            row1 = (const SkPMColor*)(srcAddr + y1 * rb);
+
+            Filter_32_opaque(subX, subY,
+                       (row0[x0]),
+                       (row0[x1]),
+                       (row1[x0]),
+                       (row1[x1]),
+                       colors);
+            colors += 1;
+            --count;
+        }
+        if (count >= 4)
+        {
+            __attribute__((aligned(16)))
+            __m128i tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, tmp8, tmp9, tmpa;
+            // load 4 pixes in each run [D, C, B, A]
+            __asm__(
+            "1:\n"
+            // load pixel A
+            "mov    (%%edx),%%edi\n"  // *xy
+            "mov    %%edi,%%ecx\n"
+            "shr    $0x12,%%edi\n"    // *xy > 0x12
+            "and    $0x3fff,%%ecx\n"  // *xy & 0x3FFF
+            "imul   %[rb],%%edi\n"    // rb * y0
+            "imul   %[rb],%%ecx\n"    // rb * y1
+
+            "mov    0x4(%%edx),%%eax\n" // *(xy + 4)
+            "mov    %%eax,%%esi\n"
+            "shr    $0x12,%%esi\n"      // x0
+            "add    %[srcAddr],%%ecx\n" // row1.0 = srcAddr + rb * y1
+            "and    $0x3fff,%%eax\n"    // x1
+            "add    %[srcAddr],%%edi\n" // row0.0 = srcAddr + rb * y0
+
+            "movd   (%%ecx,%%esi,4),%%xmm6\n"    // A: a10
+            "movd   (%%ecx,%%eax,4),%%xmm7\n"    // A: a11
+            // load pixel B
+            "mov    0x8(%%edx),%%ecx\n"          // *(xy+8)
+            "movd   (%%edi,%%esi,4),%%xmm4\n"    // A:a00
+            "movd   (%%edi,%%eax,4),%%xmm5\n"    // A:a01
+            "mov    %%ecx,%%esi\n"
+            "and    $0x3fff,%%ecx\n"      // B:y1
+            "shr    $0x12,%%esi\n"        // B:y0
+            "imul   %[rb],%%ecx\n"        // rb * y1
+            "imul   %[rb],%%esi\n"        // rb * y0
+            "mov    0xc(%%edx),%%edi\n"
+            "mov    %%edi,%%eax\n"
+            "shr    $0x12,%%eax\n"        // B:x0
+            "add    %[srcAddr],%%ecx\n"   // B:row1.1
+            "and    $0x3fff,%%edi\n"      // B:x1
+            "add    %[srcAddr],%%esi\n"   // B:row0.1
+            "movd   (%%ecx,%%eax,4),%%xmm2\n"    // B:a10
+            "movd   (%%ecx,%%edi,4),%%xmm1\n"    // B:a11
+            // load pixel C
+            "mov    0x10(%%edx),%%ecx\n"
+            "movd   (%%esi,%%eax,4),%%xmm0\n"    // B:a00
+            "movd   (%%esi,%%edi,4),%%xmm3\n"    // B:a01
+
+            "mov    %%ecx,%%eax\n"
+            "shr    $0x12,%%eax\n"               // C:y0
+            "and    $0x3fff,%%ecx\n"             // C:y1
+            "imul   %[rb],%%eax\n"
+            "imul   %[rb],%%ecx\n"
+            // [0, Ba00, 0, Aa00]
+            "punpcklqdq %%xmm0,%%xmm4\n"
+            // [0, Ba01, 0, Aa01]
+            "punpcklqdq %%xmm3,%%xmm5\n"
+            "mov    0x14(%%edx),%%esi\n"
+            "mov    %%esi,%%edi\n"
+            "add    %[srcAddr],%%eax\n"          // C: row0
+            "add    %[srcAddr],%%ecx\n"          // C: row1
+            // [0, Ba11, 0, Aa11]
+            "punpcklqdq %%xmm1,%%xmm7\n"
+            // [0, Ba10, 0, Aa10]
+            "punpcklqdq %%xmm2,%%xmm6\n"
+            "and    $0x3fff,%%esi\n"
+            "shr    $0x12,%%edi\n"
+            // [0, 0, 0, Ca01]
+            "movd   0x0(%%eax,%%esi,4),%%xmm3\n"
+            // [0, 0, 0, Ca11]
+            "movd   (%%ecx,%%esi,4),%%xmm2\n"
+            // load pixel D
+            "mov    0x18(%%edx),%%esi\n"
+            // [0, 0, 0, Ca00]
+            "movd   0x0(%%eax,%%edi,4),%%xmm1\n"
+            "mov    %%esi,%%eax\n"
+            "shr    $0x12,%%esi\n"            // D:y0
+            "and    $0x3fff,%%eax\n"          // D:y1
+            // [0, 0, 0, Ca10]
+            "movd   (%%ecx,%%edi,4),%%xmm0\n"
+
+            "imul   %[rb],%%eax\n"            // D:rb * y0
+            "imul   %[rb],%%esi\n"            // D:rb * y1
+
+            "movdqa %%xmm2,%[tmp6]\n"         // save D:a11
+            "movdqa %%xmm0,%[tmp5]\n"         // save D:a10
+
+
+            "mov    0x1c(%%edx),%%ecx\n"
+            "mov    %%ecx,%%edi\n"
+            "add    %[srcAddr],%%esi\n"       // D:row0
+            "and    $0x3fff,%%ecx\n"          // D:x1
+            "shr    $0x12,%%edi\n"            // D:x0
+            "add    %[srcAddr],%%eax\n"       // D:row1
+
+            "movd   (%%esi,%%ecx,4),%%xmm2\n"    // D:a01
+            "movd   (%%esi,%%edi,4),%%xmm0\n"    // D:a00
+
+            // [0, Da01, 0, Ca01]
+            "punpcklqdq %%xmm2,%%xmm3\n"
+            // [0, Da00, 0, Ca00]
+            "punpcklqdq %%xmm0,%%xmm1\n"
+
+            "mov 0x0(%%eax,%%edi,4), %%esi\n"       // D:a10
+            "mov %%esi, %[tmp8]\n"                  // save D:a10
+
+            "movd   0x0(%%eax,%%ecx,4),%%xmm2\n"    // D:a11
+            // [Da00, Ca00, Ba00, Aa00]
+            "shufps $0x88,%%xmm1,%%xmm4\n"
+            "movdqa %[tmp6],%%xmm1\n"        // C:a11
+            // [0, Da11, 0, Ca11]
+            "punpcklqdq %%xmm2,%%xmm1\n"
+            // [Da11, Ca11, Ba11, Aa11]
+            "shufps $0x88,%%xmm1,%%xmm7\n"   // a11.3210
+
+            "movdqa  (%%edx),%%xmm1\n"        // vy_d
+
+            "movaps %%xmm1,%%xmm2\n"         // vy_d
+            "shufps $0xdd,0x10(%%edx),%%xmm1\n"    // vx
+            "shufps $0x88,0x10(%%edx),%%xmm2\n"    // vy
+
+            "psrld  $0xe,%%xmm1\n"        // vx>>14
+            // [Da01, Ca01, Ba01, Aa01]
+            "shufps $0x88,%%xmm3,%%xmm5\n"
+            "psrld  $0xe,%%xmm2\n"         // vy>>14
+
+            "add    $0x20,%%edx\n"        // xy+=8
+            "movdqa %[tmp5],%%xmm3\n"     // C:a10
+            "movhpd %[tmp8],%%xmm3\n"     // CD:a10
+
+            "pand   vf,%%xmm1\n"           // vx & vf
+            "pand   vf,%%xmm2\n"           // vy & vf
+            // [Da10, Ca10, Ba10, Aa10]
+            "shufps $0x88,%%xmm3,%%xmm6\n"
+
+            "pshuflw $0xa0,%%xmm1,%%xmm3\n"
+            "pshuflw $0xa0,%%xmm2,%%xmm0\n"
+            "pshufhw $0xa0,%%xmm3,%%xmm1\n"
+            "pshufhw $0xa0,%%xmm0,%%xmm2\n"
+
+            "movdqa %%xmm1,%%xmm3\n"
+            "pmullw %%xmm2,%%xmm3\n"         // vxy
+
+            "psllw  $0x4,%%xmm2\n"           // v16y
+            "movdqa v256,%%xmm0\n"
+            "psllw  $0x4,%%xmm1\n"           // v16x
+
+            "paddw  %%xmm3,%%xmm0\n"         // v256+vxy
+            "psubw  %%xmm2,%%xmm0\n"         // v256+vxy-v16y
+            "psubw  %%xmm3,%%xmm2\n"         // v16y-vxy    vscale 2
+
+            "movaps %%xmm4,%[tmp9]\n"        // a00
+            "psubw  %%xmm1,%%xmm0\n"         // v256+vxy-v16y-v16x   vscale0
+
+            "movaps %%xmm5,%[tmpa]\n"        // a01
+
+            "psubw  %%xmm3,%%xmm1\n"         // v16x-vxy   vscale 1
+            "pand   vmask,%%xmm4\n"          // a00 & vmask
+            "pand   vmask,%%xmm5\n"          // a01 & vmask
+
+            "pmullw %%xmm0,%%xmm4\n"         // a00.l*vscale0\n
+            "pmullw %%xmm1,%%xmm5\n"         // a01.l*vscale1\n
+
+            "paddw  %%xmm5,%%xmm4\n"         // a00.l+a01.l
+            "movaps %%xmm6,%%xmm5\n"         // a10
+            "pand   vmask,%%xmm5\n"          // a10.l
+            "psrld  $0x8,%%xmm6\n"           // a10.h
+            "pmullw %%xmm2,%%xmm5\n"         // a10.l*vscale2
+            "paddw  %%xmm5,%%xmm4\n"         // a00.l+a01.l+a10.l
+            "movaps %%xmm7,%%xmm5\n"         // a11
+            "pand   vmask,%%xmm5\n"          // a11.l
+            "psrld  $0x8,%%xmm7\n"           // a11.h
+            "pmullw %%xmm3,%%xmm5\n"         // a11.l*vxy
+            "paddw  %%xmm5,%%xmm4\n"         // a00.l+a01.l+a10.l+a11.l
+            "movaps %[tmp9],%%xmm5\n"        // a00
+            "psrld  $0x8,%%xmm4\n"           // ax.l>>8
+            "psrld  $0x8,%%xmm5\n"           // a00.h
+            "pand   vmask,%%xmm5\n"          // a00.h
+            "pmullw %%xmm0,%%xmm5\n"         // a00.h*vscale0
+            "movaps %[tmpa],%%xmm0\n"        // a01
+            "psrld  $0x8,%%xmm0\n"           // a01.h
+            "mov    %[count], %%edi\n"
+            "pand   vmask,%%xmm0\n"          // a01.h
+            "pand   vmask,%%xmm6\n"          // a10.h
+            "pmullw %%xmm1,%%xmm0\n"         // a01.h * vscale1
+            "add    $0xfffffffc,%%edi\n"
+            "pmullw %%xmm2,%%xmm6\n"         // a10.h * vscale2
+            "pand   vmask,%%xmm7\n"          // a11.h
+            "paddw  %%xmm0,%%xmm5\n"         // a00.h+a01.h
+            "pmullw %%xmm3,%%xmm7\n"         // a11.h * vscale2
+            "paddw  %%xmm6,%%xmm5\n"         // a00.h+a01.h+a10.h
+            "mov    %[colors],%%ecx\n"       // colors
+            "paddw  %%xmm7,%%xmm5\n"         // a00.h+a01.h+a10.h+a11.h
+            "pand   vmask,%%xmm4\n"          // ax.l & vmask
+            "pand   vmask2,%%xmm5\n"         // ax.h & vmask2
+            "addl   $0x10,%[colors]\n"       // colors+=4
+            "por    %%xmm5,%%xmm4\n"         // ax.l|ax.h
+            "movdqu %%xmm4,(%%ecx)\n"        // store colors
+            "cmp    $0x4,%%edi\n"
+            "mov    %%edi,%[count]\n"
+            "jge    1b\n"
+            :"+d" (xy)
+            : [srcAddr] "m" (srcAddr), [colors] "m" (colors), [rb] "m" (rb),
+            [count] "m" (count),
+            [tmp1] "m" (tmp1), [tmp2] "m" (tmp2), [tmp3] "m" (tmp3), [tmp4] "m" (tmp4),
+            [tmp5] "m" (tmp5), [tmp6] "m" (tmp6), [tmp7] "m" (tmp7), [tmp8] "m" (tmp4),
+            [tmp9] "m" (tmp9), [tmpa] "m" (tmpa)
+            :"memory","ecx","esi","edi", "eax"
+        );
+        } // count >= 4
+    }
+    while (count > 0)
+    {
+        data = *xy++;
+         y0 = data >> 14;
+        y1 = data & 0x3FFF;
+        subY = y0 & 0xF;
+        y0 >>= 4;
+
+        data = *xy++;
+        x0 = data >> 14;
+        x1 = data & 0x3FFF;
+        subX = x0 & 0xF;
+        x0 >>= 4;
+
+        row0 = (const SkPMColor*)(srcAddr + y0 * rb);
+        row1 = (const SkPMColor*)(srcAddr + y1 * rb);
+
+        Filter_32_opaque(subX, subY,
+                   (row0[x0]),
+                   (row0[x1]),
+                   (row1[x0]),
+                   (row1[x1]),
+                   colors);
+        colors += 1;
+        count --;
+    }
+}
+void S32_alpha_D32_filter_DXDY_SSE2(const SkBitmapProcState& s,
+                                  const uint32_t* xy,
+                                  int count, uint32_t* colors) {
+
+    SkASSERT(count > 0 && colors != NULL);
+    SkASSERT(s.fDoFilter);
+    SkASSERT(s.fBitmap->config() == SkBitmap::kARGB_8888_Config);
+    SkASSERT(s.fAlphaScale < 256);
+    uint32_t data;
+    unsigned y0, y1, x0, x1, subX, subY;
+    const SkPMColor *row0, *row1;
+    const char* srcAddr = static_cast<const char*>(s.fBitmap->getPixels());
+    __m128i alphaScale = _mm_set1_epi16(s.fAlphaScale);
+    unsigned rb = s.fBitmap->rowBytes();
+    if (count >= 4) {
+        while (((size_t)xy & 0x0F) != 0)
+        {
+            data = *xy++;
+            y0 = data >> 14;
+            y1 = data & 0x3FFF;
+            subY = y0 & 0xF;
+            y0 >>= 4;
+
+            data = *xy++;
+            x0 = data >> 14;
+            x1 = data & 0x3FFF;
+            subX = x0 & 0xF;
+            x0 >>= 4;
+
+            row0 = (const SkPMColor*)(srcAddr + y0 * rb);
+            row1 = (const SkPMColor*)(srcAddr + y1 * rb);
+
+            Filter_32_alpha(subX, subY,
+                       (row0[x0]),
+                       (row0[x1]),
+                       (row1[x0]),
+                       (row1[x1]),
+                       colors,
+                       s.fAlphaScale);
+            colors += 1;
+            --count;
+        }
+    __m128i vf = _mm_set1_epi32(0xF);
+    __m128i vmask = _mm_set1_epi32(gMask_00FF00FF);
+    __m128i vmask2 = _mm_set1_epi32(0xff00ff00);
+    __m128i v256 = _mm_set1_epi16(256);
+    __m128i *d = reinterpret_cast<__m128i*>(colors);
+    while (count>=4) {
+        // [Bx1x0, By1y0, Ax1x0, Ay1y0]; load 4 pixes [D, C, B, A] in one run
+        __m128i vy_d = _mm_load_si128((__m128i*)xy);
+        // [Dx1x0, Dy1y0, Cx1x0, Cy1y0];
+        __m128i vx_d = _mm_load_si128((__m128i*)(xy+4));
+        // [Dy1y0, Cy1y0, By1y0, Ay1y0]
+        __m128i vy = (__m128i)_mm_shuffle_ps((__m128)vy_d,(__m128)vx_d,0x88);
+        // [Dx1x0, Cx1x0, Bx1x0, Ax1x0]
+        __m128i vx = (__m128i)_mm_shuffle_ps((__m128)vy_d,(__m128)vx_d,0xdd);
+
+        uint32_t XY = *xy++;
+        const uint32_t* row0 = (const uint32_t*)(srcAddr + (XY >> 18) * rb);
+        const uint32_t* row1 = (const uint32_t*)(srcAddr + (XY & 0x3FFF) * rb);
+
+        uint32_t XX = *xy++;    // x0:14 | 4 | x1:14
+        unsigned x0 = XX >> 18;
+        unsigned x1 = XX & 0x3FFF;
+        // [0, 0, 0, Ay0x0]
+        __m128i a00 = _mm_cvtsi32_si128(row0[x0]);
+        // [0, 0, 0, Ay0x1]
+        __m128i a01 = _mm_cvtsi32_si128(row0[x1]);
+        // [0, 0, 0, Ay1x0]
+        __m128i a10 = _mm_cvtsi32_si128(row1[x0]);
+        // [0, 0, 0, Ay1x1]
+        __m128i a11 = _mm_cvtsi32_si128(row1[x1]);
+
+        XY = *xy++;
+        row0 = (const uint32_t*)(srcAddr + (XY >> 18) * rb);
+        row1 = (const uint32_t*)(srcAddr + (XY & 0x3FFF) * rb);
+
+        XX = *xy++;    // x0:14 | 4 | x1:14
+        x0 = XX >> 18;
+        x1 = XX & 0x3FFF;
+        // [0, 0, By0x0, Ay0x0]
+        a00 = _mm_unpacklo_epi32(a00,_mm_cvtsi32_si128(row0[x0]));
+        // [0, 0, By0x1, Ay0x1]
+        a01 = _mm_unpacklo_epi32(a01,_mm_cvtsi32_si128(row0[x1]));
+        // [0, 0, By1x0, Ay1x0]
+        a10 = _mm_unpacklo_epi32(a10,_mm_cvtsi32_si128(row1[x0]));
+        // [0, 0, By1x1, Ay1x1]
+        a11 = _mm_unpacklo_epi32(a11,_mm_cvtsi32_si128(row1[x1]));
+
+        XY = *xy++;
+        row0 = (const uint32_t*)(srcAddr + (XY >> 18) * rb);
+        row1 = (const uint32_t*)(srcAddr + (XY & 0x3FFF) * rb);
+
+        XX = *xy++;    // x0:14 | 4 | x1:14
+        x0 = XX >> 18;
+        x1 = XX & 0x3FFF;
+        // [0, 0, 0, Cy0x0]
+        __m128i a00_d = _mm_cvtsi32_si128(row0[x0]);
+        // [0, 0, 0, Cy0x1]
+        __m128i a01_d = _mm_cvtsi32_si128(row0[x1]);
+        // [0, 0, 0, Cy1x0]
+        __m128i a10_d = _mm_cvtsi32_si128(row1[x0]);
+        // [0, 0, 0, Cy1x1]
+        __m128i a11_d = _mm_cvtsi32_si128(row1[x1]);
+
+        XY = *xy++;
+        row0 = (const uint32_t*)(srcAddr + (XY >> 18) * rb);
+        row1 = (const uint32_t*)(srcAddr + (XY & 0x3FFF) * rb);
+
+        XX = *xy++;    // x0:14 | 4 | x1:14
+        x0 = XX >> 18;
+        x1 = XX & 0x3FFF;
+        // [0, 0, Dy0x0, Cy0x0]
+        a00_d = _mm_unpacklo_epi32(a00_d,_mm_cvtsi32_si128(row0[x0]));
+        // [0, 0, Dy0x1, Cy0x1]
+        a01_d = _mm_unpacklo_epi32(a01_d,_mm_cvtsi32_si128(row0[x1]));
+        // [0, 0, Dy1x0, Cy1x0]
+        a10_d = _mm_unpacklo_epi32(a10_d,_mm_cvtsi32_si128(row1[x0]));
+        // [0, 0, Dy1x1, Cy1x1]
+        a11_d = _mm_unpacklo_epi32(a11_d,_mm_cvtsi32_si128(row1[x1]));
+
+        // [DsubX, CsubY, BsubY, AsubY]
+        vy = _mm_srli_epi32(vy,14);
+        vy = _mm_and_si128(vy,vf);
+
+        // [DsubX, CsubX, BsubX, AsubX]
+        vx = _mm_srli_epi32(vx,14);
+        vx = _mm_and_si128(vx,vf);
+
+        // [Dy0x0, Cy0x0, By0x0, Ay0x0]
+        a00 = _mm_unpacklo_epi64(a00,a00_d);
+        // [Dy0x1, Cy0x1, By0x1, Ay0x1]
+        a01 = _mm_unpacklo_epi64(a01,a01_d);
+        // [Dy1x0, Cy1x0, By1x0, Ay1x0]
+        a10 = _mm_unpacklo_epi64(a10,a10_d);
+        // [Dy1x1, Cy1x1, By1x1, Ay1x1]
+        a11 = _mm_unpacklo_epi64(a11,a11_d);
+
+        // [0, DsubX, 0, CsubY, BsubY, BsubY, AsubY, AsubY]
+        vy = _mm_shufflelo_epi16(vy,0xa0);
+        // [CsubY, DsubY, CsubY, CsubY, BsubY, BsubY, AsubY, AsubY]
+        vy = _mm_shufflehi_epi16(vy,0xa0);
+        vx = _mm_shufflelo_epi16(vx,0xa0);
+        // [CsubX, DsubX, CsubX, CsubX, BsubX, BsubX, AsubX, AsubX]
+        vx = _mm_shufflehi_epi16(vx,0xa0);
+
+        // unsigned xy = x * y;
+        __m128i vxy = _mm_mullo_epi16(vx,vy);
+        __m128i v16y = _mm_slli_epi16(vy,4);
+        __m128i v16x = _mm_slli_epi16(vx,4);
+        // unsigned scale = 256 - 16*y - 16*x + xy;
+        __m128i vscale = _mm_add_epi16(v256,vxy);
+        vscale = _mm_sub_epi16(vscale,v16y);
+        vscale = _mm_sub_epi16(vscale,v16x);
+
+        // uint32_t lo = (a00 & mask) * scale;
+        __m128i vlo = _mm_and_si128(a00,vmask);
+        vlo = _mm_mullo_epi16(vlo, vscale);
+
+        // uint32_t hi = ((a00 >> 8) & mask) * scale;
+        __m128i vhi = _mm_srli_epi32(a00,8);
+        vhi = _mm_and_si128(vhi,vmask);
+        vhi = _mm_mullo_epi16(vhi, vscale);
+        // scale = 16*x-xy;
+        vscale = _mm_sub_epi16(v16x,vxy);
+
+        // lo += (a01 & mask) * scale;
+        __m128i vlo2 = _mm_and_si128(a01,vmask);
+        vlo2 = _mm_mullo_epi16(vlo2, vscale);
+        vlo = _mm_add_epi16(vlo,vlo2);
+
+        // hi += ((a01 >> 8) & mask) * scale;
+        __m128i vhi2 = _mm_srli_epi32(a01,8);
+        vhi2 = _mm_and_si128(vhi2,vmask);
+        vhi2 = _mm_mullo_epi16(vhi2, vscale);
+        vhi = _mm_add_epi16(vhi,vhi2);
+
+        // scale = 16*y - xy;
+        vscale = _mm_sub_epi16(v16y,vxy);
+
+        // lo += (a10 & mask) * scale;
+        vlo2 = _mm_and_si128(a10,vmask);
+        vlo2 = _mm_mullo_epi16(vlo2, vscale);
+        vlo = _mm_add_epi16(vlo,vlo2);
+
+        // hi += ((a10 >> 8) & mask) * scale;
+        vhi2 = _mm_srli_epi32(a10,8);
+        vhi2 = _mm_and_si128(vhi2,vmask);
+        vhi2 = _mm_mullo_epi16(vhi2, vscale);
+        vhi = _mm_add_epi16(vhi,vhi2);
+
+        // lo += (a11 & mask) * xy;
+        vlo2 = _mm_and_si128(a11,vmask);
+        vlo2 = _mm_mullo_epi16(vlo2, vxy);
+        vlo = _mm_add_epi16(vlo,vlo2);
+
+        // hi += ((a11 >> 8) & mask) * xy;
+        vhi2 = _mm_srli_epi32(a11,8);
+        vhi2 = _mm_and_si128(vhi2,vmask);
+        vhi2 = _mm_mullo_epi16(vhi2, vxy);
+        vhi = _mm_add_epi16(vhi,vhi2);
+
+        // lo = (((lo >> 8) & mask) * alphaScale) >> 8 & mask;
+        vlo = _mm_srli_epi32(vlo,8);
+        vlo = _mm_mullo_epi16(_mm_and_si128(vlo,vmask), alphaScale);
+        vlo = _mm_srli_epi32(vlo,8);
+        vlo = _mm_and_si128(vlo,vmask);
+
+        // hi = (((hi >> 8) & mask) * alphaScale) >> 8 &(~mask);
+        vhi = _mm_srli_epi32(vhi,8);
+        vhi = _mm_mullo_epi16(_mm_and_si128(vhi,vmask), alphaScale);
+        vhi = _mm_and_si128(vhi, vmask2);
+
+        _mm_storeu_si128(d,_mm_or_si128(vlo,vhi));
+        d++;
+        count -= 4;
+        }
+        colors = reinterpret_cast<SkPMColor*>(d);
+    }
+    while (count > 0)
+    {
+        data = *xy++;
+        y0 = data >> 14;
+        y1 = data & 0x3FFF;
+        subY = y0 & 0xF;
+        y0 >>= 4;
+
+        data = *xy++;
+        x0 = data >> 14;
+        x1 = data & 0x3FFF;
+        subX = x0 & 0xF;
+        x0 >>= 4;
+
+        row0 = (const SkPMColor*)(srcAddr + y0 * rb);
+        row1 = (const SkPMColor*)(srcAddr + y1 * rb);
+
+        Filter_32_alpha(subX, subY,
+                       (row0[x0]),
+                       (row0[x1]),
+                       (row1[x0]),
+                       (row1[x1]),
+                       colors,
+                       s.fAlphaScale);
+        colors += 1;
+        count --;
+    }
+
+}
+
+void S32_alpha_D32_filter_DXDY_SSE2_asm(const SkBitmapProcState& s,
+                                  const uint32_t* xy,
+                                  int count, uint32_t* colors) {
+    SkASSERT(count > 0 && colors != NULL);
+    SkASSERT(s.fDoFilter);
+    SkASSERT(s.fBitmap->config() == SkBitmap::kARGB_8888_Config);
+    SkASSERT(s.fAlphaScale < 256);
+    uint32_t data;
+    unsigned y0, y1, x0, x1, subX, subY;
+    const SkPMColor *row0, *row1;
+    const char* srcAddr = static_cast<const char*>(s.fBitmap->getPixels());
+    unsigned rb = s.fBitmap->rowBytes();
+    unsigned alphaScale = s.fAlphaScale;
+    if (count >= 4) {
+        while (((size_t)xy & 0x0F) != 0)
+        {
+            data = *xy++;
+            y0 = data >> 14;
+            y1 = data & 0x3FFF;
+            subY = y0 & 0xF;
+            y0 >>= 4;
+
+            data = *xy++;
+            x0 = data >> 14;
+            x1 = data & 0x3FFF;
+            subX = x0 & 0xF;
+            x0 >>= 4;
+
+            row0 = (const SkPMColor*)(srcAddr + y0 * rb);
+            row1 = (const SkPMColor*)(srcAddr + y1 * rb);
+
+            Filter_32_alpha(subX, subY,
+                       (row0[x0]),
+                       (row0[x1]),
+                       (row1[x0]),
+                       (row1[x1]),
+                      colors,
+                       s.fAlphaScale);
+            colors += 1;
+            --count;
+        }
+
+        // BE CAREFUL, count >= 4
+        if (count >= 4)
+        {
+            __attribute__((aligned(16)))
+            __m128i tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, tmp8, tmp9, tmpa, tmpb;
+           // unsigned int tmpeax;
+            __asm__(
+           // "mov   %%eax, %[tmpeax]\n"
+           "movd   %[alphaScale], %%xmm0\n"
+           "pshuflw $0,%%xmm0,%%xmm0\n"
+           "punpcklqdq %%xmm0,%%xmm0\n"    // a00._1_0
+           "movaps %%xmm0,%[tmpb]\n"
+           "1:\n"
+           "mov    (%%edx),%%edi\n"
+           "mov    %%edi,%%ecx\n"
+           "shr    $0x12,%%edi\n"
+           "and    $0x3fff,%%ecx\n"
+           "imul   %[rb],%%edi\n"
+           "imul   %[rb],%%ecx\n"
+
+           "mov    0x4(%%edx),%%eax\n"
+           "mov    %%eax,%%esi\n"
+           "shr    $0x12,%%esi\n"
+           "add    %[srcAddr],%%ecx\n"    // row1.0
+           "and    $0x3fff,%%eax\n"
+           "add    %[srcAddr],%%edi\n"    // row0.0
+
+           "movd   (%%ecx,%%esi,4),%%xmm6\n"    // a10.0
+           "movd   (%%ecx,%%eax,4),%%xmm7\n"    // a11.0
+           "mov    0x8(%%edx),%%ecx\n"
+           "movd   (%%edi,%%esi,4),%%xmm4\n"    // a00.0
+           "movd   (%%edi,%%eax,4),%%xmm5\n"    // a01.0
+           "mov    %%ecx,%%esi\n"
+           "and    $0x3fff,%%ecx\n"
+           "shr    $0x12,%%esi\n"
+           "imul   %[rb],%%ecx\n"
+           "imul   %[rb],%%esi\n"
+
+           // "movaps %%xmm6,%[tmp1]\n"    // a10.0
+           // "movaps %%xmm7,%[tmp2]\n"    // a11.0
+
+           "mov    0xc(%%edx),%%edi\n"
+           "mov    %%edi,%%eax\n"
+           "shr    $0x12,%%eax\n"
+           "add    %[srcAddr],%%ecx\n"    // row1.1
+           "and    $0x3fff,%%edi\n"
+           "add    %[srcAddr],%%esi\n"    // row0.1
+           "movd   (%%ecx,%%eax,4),%%xmm2\n"    // a10.1
+           "movd   (%%ecx,%%edi,4),%%xmm1\n"    // a11.1
+           "mov    0x10(%%edx),%%ecx\n"
+           "movd   (%%esi,%%eax,4),%%xmm0\n"    // a00.1
+           "movd   (%%esi,%%edi,4),%%xmm3\n"    // a01.1
+
+           "mov    %%ecx,%%eax\n"
+           "shr    $0x12,%%eax\n"
+           "and    $0x3fff,%%ecx\n"
+           "imul   %[rb],%%eax\n"
+           "imul   %[rb],%%ecx\n"
+
+           "punpcklqdq %%xmm0,%%xmm4\n"    // a00._1_0
+           "punpcklqdq %%xmm3,%%xmm5\n"    // a01._1_0
+
+           // "movdqa %%xmm2,%[tmp3]\n"    // a10.1
+           // "movdqa %%xmm1,%[tmp4]\n"    // a11.1
+
+           "mov    0x14(%%edx),%%esi\n"
+           "mov    %%esi,%%edi\n"
+           "add    %[srcAddr],%%eax\n"        // row0.2
+           "add    %[srcAddr],%%ecx\n"        // row1.2
+           "punpcklqdq %%xmm1,%%xmm7\n"    // a01._1_0
+           "punpcklqdq %%xmm2,%%xmm6\n"    // a10._1_0
+           "and    $0x3fff,%%esi\n"
+           "shr    $0x12,%%edi\n"
+
+           // "movaps %%xmm6,%[tmp1]\n"    // a10.0
+
+
+           // "movaps %[tmp2],%%xmm7\n"        // a11.0
+           // "movhpd %[tmp4],%%xmm7\n"        // a11._1_0
+
+           "movd   0x0(%%eax,%%esi,4),%%xmm3\n"    // a01.2
+           "movd   (%%ecx,%%esi,4),%%xmm2\n"    // a11.2
+           "mov    0x18(%%edx),%%esi\n"
+           "movd   0x0(%%eax,%%edi,4),%%xmm1\n"    // a00.2
+           "mov    %%esi,%%eax\n"
+           "shr    $0x12,%%esi\n"
+           "and    $0x3fff,%%eax\n"
+           "movd   (%%ecx,%%edi,4),%%xmm0\n"    // a10.2
+
+           "imul   %[rb],%%eax\n"
+           "imul   %[rb],%%esi\n"
+
+           "movdqa %%xmm2,%[tmp6]\n"        // a11.2
+           "movdqa %%xmm0,%[tmp5]\n"        // a10.2
+
+
+           "mov    0x1c(%%edx),%%ecx\n"
+           "mov    %%ecx,%%edi\n"
+           "add    %[srcAddr],%%esi\n"            // row0.3
+           "and    $0x3fff,%%ecx\n"        // x1.3
+           "shr    $0x12,%%edi\n"            // x0.3
+           "add    %[srcAddr],%%eax\n"    // row1.3
+
+           "movd   (%%esi,%%ecx,4),%%xmm2\n"    // a01.3
+           "movd   (%%esi,%%edi,4),%%xmm0\n"    // a00.3
+
+           // "movdqa %%xmm2,%[tmp7]\n"        // a01.3 save
+           "punpcklqdq %%xmm2,%%xmm3\n"        // a01._3_2
+           "punpcklqdq %%xmm0,%%xmm1\n"        // a00._3_2
+
+           // "movd   0x0(%%eax,%%edi,4),%%xmm2\n"    // a10.3
+           // "movdqa %%xmm2,%[tmp8]\n"        // 10.3
+           "mov 0x0(%%eax,%%edi,4), %%esi\n"  // a10.3
+           "mov %%esi, %[tmp8]\n"             // a10.3
+
+           "movd   0x0(%%eax,%%ecx,4),%%xmm2\n"    // a11.3
+
+           "shufps $0x88,%%xmm1,%%xmm4\n"        // a00.3210
+           "movdqa %[tmp6],%%xmm1\n"        // a11.2
+
+           "punpcklqdq %%xmm2,%%xmm1\n"        // a11._3_2
+
+           "shufps $0x88,%%xmm1,%%xmm7\n"        // a11.3210
+
+           "movdqa (%%edx),%%xmm1\n"        // vy_d
+           // "movhpd %[tmp7],%%xmm3\n"        // a01._3_2
+
+           "movaps %%xmm1,%%xmm2\n"        // vy_d
+           "shufps $0xdd,0x10(%%edx),%%xmm1\n"    // vx
+           "shufps $0x88,0x10(%%edx),%%xmm2\n"    // vy
+
+           "psrld  $0xe,%%xmm1\n"        // vx>>14
+           "shufps $0x88,%%xmm3,%%xmm5\n"    // a01.3210
+           "psrld  $0xe,%%xmm2\n"        // vy>>14
+
+           // "movaps %[tmp1],%%xmm6\n"        // a10._1_0
+           "add    $0x20,%%edx\n"        // xy+=8
+           "movdqa %[tmp5],%%xmm3\n"        // a10.2
+           // "movhpd %[tmp3],%%xmm6\n"        // a10._1_0
+           "movhpd %[tmp8],%%xmm3\n"        // a10._3_2
+
+           "pand   vf,%%xmm1\n"        // vx & vf
+           "pand   vf,%%xmm2\n"        // vy & vf
+
+           "shufps $0x88,%%xmm3,%%xmm6\n"    // a10.3210
+
+           "pshuflw $0xa0,%%xmm1,%%xmm3\n"
+           "pshuflw $0xa0,%%xmm2,%%xmm0\n"
+           "pshufhw $0xa0,%%xmm3,%%xmm1\n"
+           "pshufhw $0xa0,%%xmm0,%%xmm2\n"
+
+           "movdqa %%xmm1,%%xmm3\n"
+           "pmullw %%xmm2,%%xmm3\n"        // vxy
+
+           "psllw  $0x4,%%xmm2\n"        // v16y
+           "movdqa v256,%%xmm0\n"
+           "psllw  $0x4,%%xmm1\n"        // v16x
+
+           "paddw  %%xmm3,%%xmm0\n"        // v256+vxy
+           "psubw  %%xmm2,%%xmm0\n"        // v256+vxy-v16y
+           "psubw  %%xmm3,%%xmm2\n"        // v16y-vxy    vscale 2
+
+           "movaps %%xmm4,%[tmp9]\n"        // a00
+           "psubw  %%xmm1,%%xmm0\n"        // v256+vxy-v16y-v16x   vscale0
+
+           "movaps %%xmm5,%[tmpa]\n"        // a01
+
+           "psubw  %%xmm3,%%xmm1\n"        // v16x-vxy   vscale 1
+           "pand   vmask,%%xmm4\n"        // a00 & vmask
+           "pmullw %%xmm0,%%xmm4\n"        // a00.l*vscale0\n
+           "pand   vmask,%%xmm5\n"        // a01 & vmask
+           "pmullw %%xmm1,%%xmm5\n"        // a01.l*vscale1\n
+
+           "paddw  %%xmm5,%%xmm4\n"        // a00.l+a01.l
+           "movaps %%xmm6,%%xmm5\n"        // a10
+           "pand   vmask,%%xmm5\n"        // a10.l
+           "psrld  $0x8,%%xmm6\n"        // a10.h
+           "pmullw %%xmm2,%%xmm5\n"        // a10.l*vscale2
+           "paddw  %%xmm5,%%xmm4\n"        // a00.l+a01.l+a10.l
+           "movaps %%xmm7,%%xmm5\n"        // a11
+           "pand   vmask,%%xmm5\n"        // a11.l
+           "psrld  $0x8,%%xmm7\n"        // a11.h
+           "pmullw %%xmm3,%%xmm5\n"        // a11.l*vxy
+           "paddw  %%xmm5,%%xmm4\n"        // a00.l+a01.l+a10.l+a11.l
+           "movaps %[tmp9],%%xmm5\n"        // a00
+           "psrld  $0x8,%%xmm4\n"        // ax.l>>8
+           "psrld  $0x8,%%xmm5\n"        // a00.h
+           "pand   vmask,%%xmm4\n"        // ax.l & vmask
+           "pand   vmask,%%xmm5\n"        // a00.h
+           "pmullw %%xmm0,%%xmm5\n"        // a00.h*vscale0
+           "movaps %[tmpa],%%xmm0\n"        // a01
+           "psrld  $0x8,%%xmm0\n"        // a01.h
+           "mov    %[count], %%edi\n"
+           "pand   vmask,%%xmm0\n"        // a01.h
+           "pmullw %%xmm1,%%xmm0\n"        // a01.h * vscale1
+           "pand   vmask,%%xmm6\n"        // a10.h
+           "add    $0xfffffffc,%%edi\n"
+           "pmullw %%xmm2,%%xmm6\n"        // a10.h * vscale2
+           "pand   vmask,%%xmm7\n"        // a11.h
+           "paddw  %%xmm0,%%xmm5\n"        // a00.h+a01.h
+           "pmullw %%xmm3,%%xmm7\n"        // a11.h * vscale2
+           // "movaps %[tmpb], %%xmm3\n"
+           "paddw  %%xmm6,%%xmm5\n"        // a00.h+a01.h+a10.h
+           "pmullw %[tmpb], %%xmm4\n"
+           "mov    %[colors],%%ecx\n"        // colors
+           "paddw  %%xmm7,%%xmm5\n"        // a00.h+a01.h+a10.h+a11.h
+           "psrld  $0x8,%%xmm5\n"        // ax.l>>8
+           "pand   vmask,%%xmm5\n"        // ax.l & vmask
+           "pmullw %[tmpb], %%xmm5\n"
+           "psrld  $0x8,%%xmm4\n"        // ax.l>>8
+           "pand   vmask,%%xmm4\n"        // ax.l & vmask
+           "pand   vmask2,%%xmm5\n"        // ax.h & vmask2
+           "addl   $0x10,%[colors]\n"        // colors+=4
+           "por    %%xmm5,%%xmm4\n"        // ax.l|ax.h
+           "movdqu %%xmm4,(%%ecx)\n"        // store colors
+           "cmp    $0x4,%%edi\n"
+           "mov    %%edi,%[count]\n"
+           "jge    1b\n"
+           :"+d" (xy)
+           :[srcAddr] "m" (srcAddr), [colors] "m" (colors), [rb] "m" (rb),
+           [count] "m" (count),[alphaScale] "m" (alphaScale),
+           [tmp1] "m" (tmp1), [tmp2] "m" (tmp2), [tmp3] "m" (tmp3), [tmp4] "m" (tmp4),
+           [tmp5] "m" (tmp5), [tmp6] "m" (tmp6), [tmp7] "m" (tmp7), [tmp8] "m" (tmp4),
+           [tmp9] "m" (tmp9), [tmpa] "m" (tmpa), [tmpb] "m" (tmpb)
+           :"memory","ecx","esi","edi", "eax"
+           );
+        } // count >= 4
+    }
+    while (count > 0)
+    {
+        data = *xy++;
+        y0 = data >> 14;
+        y1 = data & 0x3FFF;
+        subY = y0 & 0xF;
+        y0 >>= 4;
+
+        data = *xy++;
+        x0 = data >> 14;
+        x1 = data & 0x3FFF;
+        subX = x0 & 0xF;
+        x0 >>= 4;
+
+        row0 = (const SkPMColor*)(srcAddr + y0 * rb);
+        row1 = (const SkPMColor*)(srcAddr + y1 * rb);
+
+        Filter_32_alpha(subX, subY,
+                       (row0[x0]),
+                       (row0[x1]),
+                       (row1[x0]),
+                       (row1[x1]),
+                       colors,
+                       s.fAlphaScale);
+        colors += 1;
+        count --;
+    }
+}
diff --git a/src/opts/SkBitmapProcState_opts_SSE2.h b/src/opts/SkBitmapProcState_opts_SSE2.h
index 46e35a0..d8fb9b8 100644
--- a/src/opts/SkBitmapProcState_opts_SSE2.h
+++ b/src/opts/SkBitmapProcState_opts_SSE2.h
@@ -15,6 +15,22 @@ void S32_opaque_D32_filter_DX_SSE2(const SkBitmapProcState& s,
 void S32_alpha_D32_filter_DX_SSE2(const SkBitmapProcState& s,
                                   const uint32_t* xy,
                                   int count, uint32_t* colors);
+void S32_D16_filter_DX_SSE2(const SkBitmapProcState& s,
+                                  const uint32_t* xy,
+                                  int count, uint16_t* colors);
+void S32_opaque_D32_nofilter_DX_SSE2(const SkBitmapProcState& s,
+                                     const uint32_t* xy,
+                                     int count, uint32_t* colors);
+void S32_alpha_D32_filter_DXDY_SSE2(const SkBitmapProcState& s,const uint32_t* xy,
+                                   int count, uint32_t* colors);
+void S32_opaque_D32_filter_DXDY_SSE2(const SkBitmapProcState& s,
+                                     const uint32_t* xy, int count, uint32_t* colors);
+void S32_opaque_D32_filter_DXDY_SSE2_asm(const SkBitmapProcState& s,
+                                   const uint32_t* xy,
+                                   int count, uint32_t* colors);
+void S32_alpha_D32_filter_DXDY_SSE2_asm(const SkBitmapProcState& s,
+                                   const uint32_t* xy,
+                                   int count, uint32_t* colors);
 void Color32_SSE2(SkPMColor dst[], const SkPMColor src[], int count,
                   SkPMColor color);
 void ClampX_ClampY_filter_scale_SSE2(const SkBitmapProcState& s, uint32_t xy[],
diff --git a/src/opts/SkBitmapProcState_opts_SSE2_asm.S b/src/opts/SkBitmapProcState_opts_SSE2_asm.S
new file mode 100644
index 0000000..4102037
--- /dev/null
+++ b/src/opts/SkBitmapProcState_opts_SSE2_asm.S
@@ -0,0 +1,451 @@
+.globl S32_opaque_D32_filter_DX_SSE2_asm;
+#
+# extern "C" void S32_opaque_D32_filter_DX_SSE2_asm(const char*     srcAddr,
+#                                                   unsigned        rb,
+#                                                   const uint32_t* xy,
+#                                                   int             count,
+#                                                   uint32_t*       colors)
+#
+# This code was created by manually editing compiler generated code from
+# SkBitmapProcState_opts_SSE2.cpp:S32_opaque_D32_filter_DX_SSE2_intrinsic().
+# Please see the original function for more comments.
+#
+# Optimizations that have been made:
+# - better register allocation to remove unnecessary loads in the loop
+# - re-ordering of multiplications to distribute them more evenly
+# - unrolling the loop once and interleaving it
+#
+# These optimizations showed 40% performance improvement (i.e. 40% fewer
+# clock tics) over the compiler generated code on an Atom CPU in the tests
+# that were run.
+#
+# Register allocation in the loop:
+#   eax     - row0
+#   ebx     - row1
+#   ecx     - scratch (x0)
+#   edx     - scratch (x1)
+#   esi     - source
+#   edi     - destination
+#
+#   xmm0..3 - scratch
+#   xmm4    - zero
+#   xmm5    - terminal value for source (source + 4 * count)
+#   xmm6    - sixteen
+#   xmm7    - allY
+#
+
+S32_opaque_D32_filter_DX_SSE2_asm:
+
+      push   %ebp
+      pxor   %xmm4,%xmm4           # _mm_setzero_si128()
+      mov    %esp,%ebp
+      push   %esi
+      push   %edi
+      push   %ebx
+      sub    $0xc,%esp
+
+      mov    0x10(%ebp),%esi       # xy
+      mov    0xc(%ebp),%edx        # rb
+
+      mov    $0x10, %ebx
+      movd   %ebx, %xmm7
+      pshuflw $0x0,%xmm7,%xmm6     # sixteen = _mm_shufflelo_epi16(sixteen, 0)
+
+      mov    0x8(%ebp),%ebx        # srcAddr
+      pshufd $0x0,%xmm6,%xmm6      # sixteen = _mm_shuffle_epi32(sixteen, 0)
+      mov    (%esi),%edi           # XY = *xy
+      mov    %edi,%eax             # XY
+      shr    $0xe,%eax             # y0 = XY >> 14
+      mov    %eax,-0x14(%ebp)      # y0
+      mov    %edi,%eax             # XY
+      and    $0x3fff,%edi          # XY & 0x3FFF
+      shr    $0x12,%eax            # y0 >> 4 == XY >> 18
+      imul   %edx,%eax             # (y0 >> 4) * rb
+      imul   %edx,%edi             # (XY & 0x3FFF) * rb
+      andl   $0xf,-0x14(%ebp)      # subY = y0 & 0xF
+      lea    (%ebx,%eax,1),%eax    # row0 = srcAddr + (y0 >> 4) * rb
+      lea    (%ebx,%edi,1),%ebx    # row1 = srcAddr + (XY & 0x3FFF) * rb
+
+      movd   -0x14(%ebp),%xmm7     # __m128i allY = _mm_cvtsi32_si128(subY)
+
+      pshuflw $0x0,%xmm7,%xmm7     # allY = _mm_shufflelo_epi16(allY, 0)
+      movdqa %xmm6,%xmm0
+      psubw  %xmm7,%xmm0           # __m128i negY = _mm_sub_epi16(sixteen, allY)
+      punpcklqdq %xmm0,%xmm7       # allY = _mm_unpacklo_epi64(allY, negY)
+
+      mov    0x18(%ebp),%edi       # colors
+
+      lea    -0x4(%edi),%edi
+
+      mov    0x14(%ebp),%ecx       # count
+      mov    %ecx,%edx
+      shl    $0x2,%ecx             # 4 * count
+      add    %esi,%ecx             # xy + 4 * count
+      movd   %ecx,%xmm5            # terminal source (xy + 4 * count)
+
+      and    $1,%edx
+      test   %edx,%edx
+      jz     evenloop              # even count; go to the beginning of
+                                   # the unrolled loop
+
+
+          # odd count; prepare for entering the unrolled loop in the middle
+          mov    0x4(%esi),%edx      # *xy == XX
+
+          movdqa %xmm6,%xmm1         # sixteen
+
+          movd   %edx,%xmm3          # XX
+          mov    %edx,%ecx           # XX
+          and    $0x3fff,%edx        # x1 = XX & 0x3FFF
+          shr    $0x12,%ecx          # x0 = XX >> 18
+          pslld  $0xe,%xmm3          # XX << 14
+          psrld  $0x1c,%xmm3         # (XX >> 14) & 0xF
+
+          movd   (%eax,%ecx,4),%xmm2 # a00 = row0[x0]
+
+          jmp odd                    # enter the unrolled loop
+                                     # in the middle
+
+
+      nop                        # for loop alignment
+      nop
+      nop
+
+evenloop:
+      mov    0x4(%esi),%edx      # *xy == XX
+
+      movdqa %xmm6,%xmm1         # sixteen
+
+      movd   %edx,%xmm3          # XX
+      mov    %edx,%ecx           # XX
+      and    $0x3fff,%edx        # x1 = XX & 0x3FFF
+      shr    $0x12,%ecx          # x0 = XX >> 18
+      pslld  $0xe,%xmm3          # XX << 14
+      psrld  $0x1c,%xmm3         # (XX >> 14) & 0xF
+
+      movd   (%ebx,%ecx,4),%xmm0 # a10 = row1[x0]
+      movd   (%eax,%ecx,4),%xmm2 # a00 = row0[x0]
+
+      pshuflw $0x0,%xmm3,%xmm3   # allX = _mm_shufflelo_epi16(allX, 0)
+      pshufd $0x0,%xmm3,%xmm3    # allX = _mm_shuffle_epi32(allX, 0)
+      psubw  %xmm3,%xmm1         # negX = _mm_sub_epi16(sixteen, allX)
+
+      pmullw %xmm7,%xmm1         # allY * negX
+      punpckldq %xmm2,%xmm0      # a00a10 = _mm_unpacklo_epi32(a10, a00)
+      pmullw %xmm7,%xmm3         # allY * allX
+      punpcklbw %xmm4,%xmm0      # a00a10 = _mm_unpacklo_epi8(a00a10, zero)
+
+      movd   (%ebx,%edx,4),%xmm2 # a11 = row1[x1]
+
+      pmullw %xmm1,%xmm0         # a00a10 = a0010 * negX * allY
+
+      lea    0x4(%esi),%esi
+      movd   (%eax,%edx,4),%xmm1 # a01 = row0[x1]
+
+          mov    0x4(%esi),%edx      # *xy == XX (the first unrolled instrctn)
+
+      punpckldq %xmm1,%xmm2      # a01a11 = _mm_unpacklo_epi32(a11, a01)
+
+          movdqa %xmm6,%xmm1         # sixteen
+
+      punpcklbw %xmm4,%xmm2      # a01a11 = _mm_unpacklo_epi8(a01a11, zero)
+      pmullw %xmm3,%xmm2         # a01a11 = a01a11 * allY * allX
+
+          movd   %edx,%xmm3          # XX
+          mov    %edx,%ecx           # XX
+          and    $0x3fff,%edx        # x1 = XX & 0x3FFF
+          shr    $0x12,%ecx          # x0 = XX >> 18
+          pslld  $0xe,%xmm3          # XX << 14
+          psrld  $0x1c,%xmm3         # (XX >> 14) & 0xF
+
+      paddw  %xmm2,%xmm0         # sum = _mm_add_epi16(a00a10, a01a11)
+      pshufd $0xee,%xmm0,%xmm2   # shifted = _mm_shuffle_epi32(sum, 0xEE)
+      paddw  %xmm2,%xmm0         # sum = _mm_add_epi16(sum, shifted)
+
+          movd   (%eax,%ecx,4),%xmm2 # a00 = row0[x0]
+
+      lea    0x4(%edi),%edi
+      psrlw  $0x8,%xmm0          # sum = _mm_srli_epi16(sum, 8)
+      packuswb %xmm4,%xmm0       # sum = _mm_packus_epi16(sum, zero)
+
+      movd   %xmm0,(%edi,1)      # *colors = _mm_cvtsi128_si32(sum)
+
+
+odd:
+          movd   (%ebx,%ecx,4),%xmm0 # a10 = row1[x0]
+
+          pshuflw $0x0,%xmm3,%xmm3   # allX = _mm_shufflelo_epi16(allX, 0)
+          pshufd $0x0,%xmm3,%xmm3    # allX = _mm_shuffle_epi32(allX, 0)
+          psubw  %xmm3,%xmm1         # negX = _mm_sub_epi16(sixteen, allX)
+
+          pmullw %xmm7,%xmm1         # allY * negX
+          punpckldq %xmm2,%xmm0      # a00a10 = _mm_unpacklo_epi32(a10, a00)
+          pmullw %xmm7,%xmm3         # allY * allX
+          punpcklbw %xmm4,%xmm0      # a00a10 = _mm_unpacklo_epi8(a00a10, zero)
+
+          movd   (%ebx,%edx,4),%xmm2 # a11 = row1[x1]
+
+          pmullw %xmm1,%xmm0         # a00a10 = a0010 * negX * allY
+
+          movd   (%eax,%edx,4),%xmm1 # a01 = row0[x1]
+          lea    0x4(%esi),%esi
+          punpckldq %xmm1,%xmm2      # a01a11 = _mm_unpacklo_epi32(a11, a01)
+          punpcklbw %xmm4,%xmm2      # a01a11 = _mm_unpacklo_epi8(a01a11, zero)
+
+          pmullw %xmm3,%xmm2         # a01a11 = a01a11 * allY * allX
+
+      movd   %xmm5,%ecx          # terminal value for source
+
+          paddw  %xmm2,%xmm0         #  sum = _mm_add_epi16(a00a10, a01a11)
+          pshufd $0xee,%xmm0,%xmm2   #  shifted = _mm_shuffle_epi32(sum, 0xEE)
+          paddw  %xmm2,%xmm0         #  sum = _mm_add_epi16(sum, shifted)
+
+      sub    %esi,%ecx           # compare source to its terminal value
+
+          lea    0x4(%edi),%edi
+          psrlw  $0x8,%xmm0          # sum = _mm_srli_epi16(sum, 8)
+          packuswb %xmm4,%xmm0       # sum = _mm_packus_epi16(sum, zero)
+
+          movd   %xmm0,(%edi,1)      # *colors = _mm_cvtsi128_si32(sum)
+
+      jnz    evenloop            # loop if source not yet at its terminal value
+
+
+      add    $0xc,%esp
+      pop    %ebx
+      pop    %edi
+      pop    %esi
+      pop    %ebp
+      ret
+
+
+.globl S32_opaque_D32_nofilter_DX_SSE2_asm;
+S32_opaque_D32_nofilter_DX_SSE2_asm:
+
+    push %ebp
+    mov  %esp, %ebp
+    push %esi
+    push %edi
+    push %ebx
+    sub  $0x8, %esp
+
+    mov 0x08(%ebp), %esi           # xy
+    mov 0x0c(%ebp), %ecx           # count
+    mov 0x0c(%ebp), %ecx           # count
+    mov 0x10(%ebp), %edx           # srcAddr
+    mov 0x14(%ebp), %edi           # colors
+
+    sarl $0x2, %ecx                # count / 4
+    test %ecx, %ecx
+    jle nofilter_done
+
+    sall $0x3, %ecx                # terminal value for index
+    movd %ecx, %xmm7
+
+    xor %eax, %eax                 # index = 0
+
+    # prepare for part 1
+    movl (%esi,%eax,1), %ebx
+
+    shr $0x4, %ecx                 # count / 8
+    jc .Lpart1odd
+    jmp .Lpart1even
+
+
+    .align 16
+.Lloop:
+    # part 2
+    movzx %cx, %ebx
+    shr $0x10, %ecx
+    movl (%edx,%ebx,4), %ebx
+    movl (%edx,%ecx,4), %ecx
+    movl %ebx, -0x8(%edi,%eax,2)
+    movl (%esi,%eax,1), %ebx       # prepare for part 1
+    movl %ecx, -0x4(%edi,%eax,2)
+
+.Lpart1even:
+    # part 1
+    movzx %bx, %ecx
+    shr $0x10, %ebx
+    movl (%edx,%ecx,4), %ecx
+    movl (%edx,%ebx,4), %ebx
+    movl %ecx, (%edi,%eax,2)
+    movl 0x4(%esi,%eax,1), %ecx    # prepare for part 2
+    movl %ebx, 0x4(%edi,%eax,2)
+
+    lea 0x8(%eax), %eax
+
+    # part 2
+    movzx %cx, %ebx
+    shr $0x10, %ecx
+    movl (%edx,%ebx,4), %ebx
+    movl (%edx,%ecx,4), %ecx
+    movl %ebx, -0x8(%edi,%eax,2)
+    movl (%esi,%eax,1), %ebx       # prepare for part 1
+    movl %ecx, -0x4(%edi,%eax,2)
+
+.Lpart1odd:
+    # part 1
+    movzx %bx, %ecx
+    shr $0x10, %ebx
+    movl (%edx,%ecx,4), %ecx
+    movl (%edx,%ebx,4), %ebx
+    movl %ecx, (%edi,%eax,2)
+    movl 0x4(%esi,%eax,1), %ecx    # prepare for part 2
+    movl %ebx, 0x4(%edi,%eax,2)
+
+    lea 0x8(%eax), %eax
+
+    movd %xmm7, %ebx
+    sub %eax, %ebx
+
+    jnz .Lloop
+
+
+    # part 2
+    movzx %cx, %ebx
+    shr $0x10, %ecx
+    movl (%edx,%ebx,4), %ebx
+    movl (%edx,%ecx,4), %ecx
+    movl %ebx, -0x8(%edi,%eax,2)
+    movl %ecx, -0x4(%edi,%eax,2)
+
+
+nofilter_done:
+    add  $0x8, %esp
+    pop  %ebx
+    pop  %edi
+    pop  %esi
+    pop  %ebp
+    ret
+
+# extern "C" void S32_Opaque_D32_filter_line_SSSE3_asm(const unsigned int* row0,
+#                                       const unsigned int* row1,
+#                                       SkFixed fx,
+#                                       unsigned int subY,
+#                                       unsigned int* colors,
+#                                       SkFixed dx,
+#                                       int count);
+
+.globl S32_Opaque_D32_filter_line_SSSE3_asm;
+S32_Opaque_D32_filter_line_SSSE3_asm:
+    push %ebp
+    mov  %esp, %ebp
+    push    %esi
+    push    %edi
+    push    %ebx
+    lea    -0xc(%esp),%esp
+
+
+    mov    0x10(%ebp),%eax
+    mov    %eax,%edi
+    shr    $0xc,%edi
+    and    $0xf,%edi
+    movd   %edi,%xmm4
+    pshuflw $0x0,%xmm4,%xmm6 ## (0, 0, 0, 0,x,x,x,x)
+
+    mov    $0x10,%ecx
+    movd   %ecx,%xmm0
+    pshuflw $0x0,%xmm0,%xmm1
+    movdqa %xmm1,%xmm3      ##xmm1 = sixteen
+    psubw  %xmm6,%xmm3      ##(0, 0, 0, 0, 16-x, 16-x, 16-x, 16-x)
+    punpcklqdq %xmm6,%xmm3  ##(x,x,x,x, 16-x,16-x,16-x,16-x)
+
+    ##subY << 8 | 16-subY
+    mov    0x14(%ebp),%ecx
+    mov    %ecx,%edx
+    shl    $0x8,%ecx
+
+    neg    %edx
+    add    $0x10,%edx
+    or     %edx,%ecx
+    movd   %ecx,%xmm0 ## allY = _mm_cvtsi32_si128((subY << 8) | (16 - subY))
+    pshuflw $0x0,%xmm0,%xmm0
+    pshufd $0x0,%xmm0,%xmm0  #xmm0 = allY
+
+    mov    0x8(%ebp),%eax   #eax: row0
+    mov    0xc(%ebp),%edx   #edx: row1
+    mov    0x18(%ebp),%esi    #esi: colors
+
+    mov    0x20(%ebp),%ebx     #ebp: count
+    mov    %ebx, %ecx          #unroll the loop,count = count/2
+    shr    $0x1, %ebx          #unroll the loop,count = count/2
+    and    $1,%ecx             #if count = odd, go odd first and even loop
+
+    jz     .LevenPrepare
+
+        mov    0x1c(%ebp),%ecx   #ecx: dx
+        mov    0x10(%ebp),%ebp
+        mov    %ebp,%edi
+        sar    $0x10,%edi               #fx>>16
+        movdqa %xmm3,%xmm5
+        jmp     .Loddloop
+
+.LevenPrepare:
+    mov    0x1c(%ebp),%ecx   #ecx: dx
+    mov    0x10(%ebp),%ebp
+    mov    %ebp,%edi
+    sar    $0x10,%edi               #fx>>16
+
+.align 16
+.Levenloop:
+    movq   (%eax,%edi,4),%xmm4     #a01a00
+    dec    %ebx                    #ebx = count --
+    movq   (%edx,%edi,4),%xmm2     #a11a10
+    punpcklbw %xmm2,%xmm4          #a01a00 = _mm_unpacklo_epi8(a01a00, a11a10);
+    pmaddubsw %xmm0,%xmm4          #sum = _mm_maddubs_epi16(a01a00, allY);
+    pmullw %xmm3,%xmm4             #sum = _mm_mullo_epi16(sum, negX);
+    add    %ecx,%ebp               #fx = fx+dx
+    pshufd $0xe,%xmm4,%xmm3        #shifted = _mm_shuffle_epi32(sum, 0xE);
+
+    paddw  %xmm3,%xmm4             #sum = _mm_add_epi16(sum, shifted);
+    mov    %ebp,%edi
+
+    psrlw  $0x8,%xmm4              #sum = _mm_srli_epi16(sum, 8)
+    shr    $0xc,%edi               #fx>>12 & 0xF
+    packuswb %xmm3,%xmm4           #sum = _mm_packus_epi16(sum, shifted)
+    and    $0xf,%edi
+    movd   %edi,%xmm5              #allX = _mm_cvtsi32_si128(subX);
+    movd   %xmm4,(%esi)            #*colors = _mm_cvtsi128_si32(sum)
+    add    $0x4,%esi               #colors++
+
+    mov    %ebp,%edi
+    sar    $0x10,%edi               #fx>>16
+.Loddloop:
+    movq   (%eax,%edi,4),%xmm2
+    movq   (%edx,%edi,4),%xmm6
+    punpcklbw %xmm6,%xmm2
+
+    pmaddubsw %xmm0,%xmm2
+    movdqa %xmm1,%xmm6
+    pshuflw $0x0,%xmm5,%xmm7       #allX = _mm_shufflelo_epi16(allX, 0)
+    psubw  %xmm7,%xmm6             #negX = _mm_sub_epi16(sixteen, allX)
+    punpcklqdq %xmm7,%xmm6         #negX = _mm_unpacklo_epi64(negX, allX)
+    add    %ecx,%ebp               #fx = fx + dx
+    pmullw %xmm6,%xmm2             #sum = _mm_mullo_epi16(sum, negX)
+    mov    %ebp,%edi
+    shr    $0xc,%edi               #fx>>12 & 0xF
+
+    and    $0xf,%edi
+    pshufd $0xe,%xmm2,%xmm7
+    paddw  %xmm7,%xmm2
+    psrlw  $0x8,%xmm2
+    movd   %edi,%xmm3
+    mov    %ebp,%edi
+    pshuflw $0x0,%xmm3,%xmm4
+    movdqa %xmm1,%xmm3
+    packuswb %xmm7,%xmm2
+    psubw  %xmm4,%xmm3
+    movd   %xmm2,(%esi)
+    add    $0x4,%esi            #colors = colors + 8
+    sar    $0x10,%edi           #fx >> 16
+    punpcklqdq %xmm4,%xmm3
+    test   %ebx,%ebx
+    jg     .Levenloop
+
+    lea    0xc(%esp),%esp
+    pop    %ebx
+    pop    %edi
+    pop    %esi
+    pop    %ebp
+    ret
diff --git a/src/opts/opts_check_SSE2.cpp b/src/opts/opts_check_SSE2.cpp
index c8d6f55..e624d06 100644
--- a/src/opts/opts_check_SSE2.cpp
+++ b/src/opts/opts_check_SSE2.cpp
@@ -138,6 +138,12 @@ void SkBitmapProcState::platformProcs() {
             fSampleProc32 = S32_opaque_D32_filter_DX_SSE2;
         } else if (fSampleProc32 == S32_alpha_D32_filter_DX) {
             fSampleProc32 = S32_alpha_D32_filter_DX_SSE2;
+        } else if (fSampleProc32 == S32_opaque_D32_nofilter_DX) {
+            fSampleProc32 = S32_opaque_D32_nofilter_DX_SSE2;
+        } else if (fSampleProc32 == S32_opaque_D32_filter_DXDY) {
+            fSampleProc32 = S32_opaque_D32_filter_DXDY_SSE2_asm;
+        } else if (fSampleProc32 == S32_alpha_D32_filter_DXDY) {
+            fSampleProc32 = S32_alpha_D32_filter_DXDY_SSE2_asm;
         }
 
         if (fSampleProc16 == S32_D16_filter_DX) {
-- 
2.7.4

